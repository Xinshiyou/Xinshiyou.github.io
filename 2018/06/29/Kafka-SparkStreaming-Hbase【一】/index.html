<!DOCTYPE html><html lang="zh-CN"><head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="xinshiyou">
  <meta name="keywords" content="">
  <title>Kafka-&gt;SparkStreaming-&gt;Hbase【一】 - 星空捞月：找寻心中的安宁</title>

  


  
  

  
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->










<!-- 自定义样式保持在最底部 -->


  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星空捞月：找寻心中的安宁" type="application/atom+xml">
<script>function loadCss(l){var d=document,h=d.head,s=d.createElement('link');s.rel='stylesheet';s.href=l;!function e(f){if (d.body)return f();setTimeout(function(){e(f)})}(function(){h.appendChild(s);});}loadCss('/style.css');loadCss('https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css');loadCss('https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css');loadCss('https://cdn.staticfile.org/highlight.js/10.0.0/styles/tomorrow-night-blue.min.css');loadCss('//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css');loadCss('//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css');loadCss('https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css');</script><noscript><link rel="stylesheet" href="/style.css"><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/tomorrow-night-blue.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"></noscript></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>星空捞月：找寻心中的安宁</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax="true" style="background: url('/img/top.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2018-06-29 16:52">
      2018年6月29日 下午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      57
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>  根据业务需求，将Kafka中数据抽取插入到Hbase中。目前网上可以找到许多相关的文章，这里介绍Github上的一个开源工具。</p>
<a id="more"></a>

<h2 id="工具地址"><a href="#工具地址" class="headerlink" title="工具地址"></a>工具地址</h2><h3 id="Github上搜索结果"><a href="#Github上搜索结果" class="headerlink" title="Github上搜索结果"></a>Github上搜索结果</h3><p><img src="001.png" srcset="/img/loading.gif" alt="这里写图片描述"></p>
<h3 id="选择工具"><a href="#选择工具" class="headerlink" title="选择工具"></a>选择工具</h3><p><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="noopener">SparkOnHbase</a></p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><ol>
<li>Hadoop: 2.6.0-cdh5.12.1</li>
<li>Spark:   1.6.0-2.10.5</li>
<li>Hbase:  1.2.0-cdh5.12.1</li>
<li>Hive:     1.1.0-cdh5.12.1</li>
<li>Kafka:   kafka_2.10-0.9.0</li>
<li>OS:       CentOS Linux release 7.4.1708 (Core)</li>
<li>JDK:      jdk1.8.0_151</li>
</ol>
<h2 id="场景需求"><a href="#场景需求" class="headerlink" title="场景需求"></a>场景需求</h2><ol>
<li>支持自定义接口解析Kafka对象</li>
<li>支持插入不同的Hbase表，即配置多个Hbase表名</li>
<li>Kafka偏移量，可以写入Zookeeper，自定义偏移量</li>
</ol>
<h2 id="工具研究"><a href="#工具研究" class="headerlink" title="工具研究"></a>工具研究</h2><h3 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h3><p>  对于Scala用户，主要是以下源代码</p>
<div class="hljs"><pre class=" language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params">@transient sc: <span class="hljs-type"><code class="language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params">@transient sc: <span class="hljs-type">SparkContext</span>,</span></span>
<span class="hljs-class"><span class="hljs-params">                   @transient config: <span class="hljs-type">Configuration</span>,</span></span>
<span class="hljs-class"><span class="hljs-params">                    val tmpHdfsConfgFile: <span class="hljs-type">String</span> = null</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Logging</span> </span>{


  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()
  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> tmpHdfsConfiguration:<span class="hljs-type">Configuration</span> = config
  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> appliedCredentials = <span class="hljs-literal">false</span>;
  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> job = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(config)
  <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)
  <span class="hljs-keyword">val</span> broadcastedConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(config))
  <span class="hljs-keyword">val</span> credentialsConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(job.getCredentials()))

  <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span> &amp;&amp; config != <span class="hljs-literal">null</span>) {
    <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(config)
    <span class="hljs-keyword">val</span> tmpPath = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile)
    <span class="hljs-keyword">if</span> (!fs.exists(tmpPath)) {
      <span class="hljs-keyword">val</span> outputStream = fs.create(tmpPath)
      config.write(outputStream)
      outputStream.close();
    } <span class="hljs-keyword">else</span> {
      logWarning(<span class="hljs-string">"tmpHdfsConfigDir "</span> + tmpHdfsConfgFile + <span class="hljs-string">" exist!!"</span>)
    }
  }


  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD foreachPartition.</span>
<span class="hljs-comment">   * This function differs from the original in that it offers the</span>
<span class="hljs-comment">   * developer access to a already connected HConnection object</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span>
<span class="hljs-comment">   * management is handled outside this method</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param f    Function to be given a iterator to iterate through</span>
<span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span>
<span class="hljs-comment">   *             with HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachPartition</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],
                          f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {
    rdd.foreachPartition(
      it =&gt; hbaseForeachPartition(broadcastedConf, it, f))
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming dStream foreach</span>
<span class="hljs-comment">   * This function differs from the original in that it offers the</span>
<span class="hljs-comment">   * developer access to a already connected HConnection object</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span>
<span class="hljs-comment">   * management is handled outside this method</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param f        Function to be given a iterator to iterate through</span>
<span class="hljs-comment">   *                 the DStream values and a HConnection object to</span>
<span class="hljs-comment">   *                 interact with HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachRDD</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                    f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {
    dstream.foreach((rdd, time) =&gt; {
      foreachPartition(rdd, f)
    })
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD mapPartition.</span>
<span class="hljs-comment">   * This function differs from the original in that it offers the</span>
<span class="hljs-comment">   * developer access to a already connected HConnection object</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span>
<span class="hljs-comment">   * management is handled outside this method</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span>
<span class="hljs-comment">   *       getting data from HBase</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param mp   Function to be given a iterator to iterate through</span>
<span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span>
<span class="hljs-comment">   *             with HBase</span>
<span class="hljs-comment">   * @return     Returns a new RDD generated by the user definition</span>
<span class="hljs-comment">   *             function just like normal mapPartition</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mapPartition</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>: <span class="hljs-type">ClassTag</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],
                                   mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">R</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">R</span>] = {

    rdd.mapPartitions[<span class="hljs-type">R</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>](broadcastedConf,
      it,
      mp), <span class="hljs-literal">true</span>)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming DStream</span>
<span class="hljs-comment">   * mapPartition.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * This function differs from the original in that it offers the</span>
<span class="hljs-comment">   * developer access to a already connected HConnection object</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span>
<span class="hljs-comment">   * management is handled outside this method</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span>
<span class="hljs-comment">   *       getting data from HBase</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param mp       Function to be given a iterator to iterate through</span>
<span class="hljs-comment">   *                 the DStream values and a HConnection object to</span>
<span class="hljs-comment">   *                 interact with HBase</span>
<span class="hljs-comment">   * @return         Returns a new DStream generated by the user</span>
<span class="hljs-comment">   *                 definition function just like normal mapPartition</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamMap</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                                mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {

    dstream.mapPartitions(it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](
      broadcastedConf,
      it,
      mp), <span class="hljs-literal">true</span>)
  }



  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take RDD</span>
<span class="hljs-comment">   * and generate puts and send them to HBase.</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to put into</span>
<span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a HBase Put</span>
<span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>, autoFlush: <span class="hljs-type">Boolean</span>) {

    rdd.foreachPartition(
      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](
        broadcastedConf,
        it,
        (iterator, hConnection) =&gt; {
          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)
          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)
          iterator.foreach(<span class="hljs-type">T</span> =&gt; htable.put(f(<span class="hljs-type">T</span>)))
          htable.flushCommits()
          htable.close()
        }))
  }

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">applyCreds</span></span>[<span class="hljs-type">T</span>] (configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]){


    credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()

    logInfo(<span class="hljs-string">"appliedCredentials:"</span> + appliedCredentials + <span class="hljs-string">",credentials:"</span> + credentials);

    <span class="hljs-keyword">if</span> (appliedCredentials == <span class="hljs-literal">false</span> &amp;&amp; credentials != <span class="hljs-literal">null</span>) {
      appliedCredentials = <span class="hljs-literal">true</span>
      logCredInformation(credentials)

      <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> ugi = <span class="hljs-type">UserGroupInformation</span>.getCurrentUser();
      ugi.addCredentials(credentials)
      <span class="hljs-comment">// specify that this is a proxy user</span>
      ugi.setAuthenticationMethod(<span class="hljs-type">AuthenticationMethod</span>.<span class="hljs-type">PROXY</span>)

      ugi.addCredentials(credentialsConf.value.value)
    }
  }

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logCredInformation</span></span>[<span class="hljs-type">T</span>] (credentials2:<span class="hljs-type">Credentials</span>) {
    logInfo(<span class="hljs-string">"credentials:"</span> + credentials2);
    <span class="hljs-keyword">for</span> (a &lt;- <span class="hljs-number">0</span> until credentials2.getAllSecretKeys.size()) {
      logInfo(<span class="hljs-string">"getAllSecretKeys:"</span> + a + <span class="hljs-string">":"</span> + credentials2.getAllSecretKeys.get(a));
    }
    <span class="hljs-keyword">val</span> it = credentials2.getAllTokens.iterator();
    <span class="hljs-keyword">while</span> (it.hasNext) {
      logInfo(<span class="hljs-string">"getAllTokens:"</span> + it.next());
    }
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generate puts and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName  The name of the table to put into</span>
<span class="hljs-comment">   * @param f          Function to convert a value in</span>
<span class="hljs-comment">   *                   the DStream to a HBase Put</span>
<span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                       tableName: <span class="hljs-type">String</span>,
                       f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>,
                       autoFlush: <span class="hljs-type">Boolean</span>) = {
    dstream.foreach((rdd, time) =&gt; {
      bulkPut(rdd, tableName, f, autoFlush)
    })
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take RDD</span>
<span class="hljs-comment">   * and generate checkAndPuts and send them to HBase.</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to put into</span>
<span class="hljs-comment">   * @param f         Function to convert a value in the RDD to</span>
<span class="hljs-comment">   *                  a HBase checkAndPut</span>
<span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {
    rdd.foreachPartition(
      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](
        broadcastedConf,
        it,
        (iterator, hConnection) =&gt; {


          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)
          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)

          iterator.foreach(<span class="hljs-type">T</span> =&gt; {
            <span class="hljs-keyword">val</span> checkPut = f(<span class="hljs-type">T</span>)
            htable.checkAndPut(checkPut._1, checkPut._2, checkPut._3, checkPut._4, checkPut._5)
          })
          htable.flushCommits()
          htable.close()
        }))
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generate checkAndPuts and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName  The name of the table to checkAndPut into</span>
<span class="hljs-comment">   * @param f          function to convert a value in the RDD to</span>
<span class="hljs-comment">   *                   a HBase checkAndPut</span>
<span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {
    dstream.foreach((rdd, time) =&gt; {
      bulkCheckAndPut(rdd, tableName, f, autoFlush)
    })
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a RDD and</span>
<span class="hljs-comment">   * generate increments and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to increment to</span>
<span class="hljs-comment">   * @param f         function to convert a value in the RDD to a</span>
<span class="hljs-comment">   *                  HBase Increments</span>
<span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkIncrement</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>, batchSize: <span class="hljs-type">Integer</span>) {
    bulkMutation(rdd, tableName, f, batchSize)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate delete</span>
<span class="hljs-comment">   * and send them to HBase.  The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to delete from</span>
<span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span>
<span class="hljs-comment">   *                  HBase Deletes</span>
<span class="hljs-comment">   * @param batchSize       The number of delete to batch before sending to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>, batchSize: <span class="hljs-type">Integer</span>) {
    bulkMutation(rdd, tableName, f, batchSize)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate</span>
<span class="hljs-comment">   * checkAndDelete and send them to HBase.  The complexity of managing the</span>
<span class="hljs-comment">   * HConnection is removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to delete from</span>
<span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span>
<span class="hljs-comment">   *                  HBase Deletes</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],
                         tableName: <span class="hljs-type">String</span>,
                         f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {
    rdd.foreachPartition(
      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](
        broadcastedConf,
        it,
        (iterator, hConnection) =&gt; {
          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)

          iterator.foreach(<span class="hljs-type">T</span> =&gt; {
            <span class="hljs-keyword">val</span> checkDelete = f(<span class="hljs-type">T</span>)
            htable.checkAndDelete(checkDelete._1, checkDelete._2, checkDelete._3, checkDelete._4, checkDelete._5)
          })
          htable.flushCommits()
          htable.close()
        }))
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generate Increments and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to increments into</span>
<span class="hljs-comment">   * @param f         Function to convert a value in the DStream to a</span>
<span class="hljs-comment">   *                  HBase Increments</span>
<span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkIncrement</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                             tableName: <span class="hljs-type">String</span>,
                             f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>,
                             batchSize: <span class="hljs-type">Int</span>) = {
    streamBulkMutation(dstream, tableName, f, batchSize)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generate Delete and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName  The name of the table to delete from</span>
<span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span>
<span class="hljs-comment">   *                   HBase Delete</span>
<span class="hljs-comment">   * @param batchSize        The number of deletes to batch before sending to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                          tableName: <span class="hljs-type">String</span>,
                          f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>,
                          batchSize: <span class="hljs-type">Integer</span>) = {
    streamBulkMutation(dstream, tableName, f, batchSize)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the bulkCheckDelete method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generate CheckAndDelete and send them to HBase.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * The complexity of managing the HConnection is</span>
<span class="hljs-comment">   * removed from the developer</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName  The name of the table to delete from</span>
<span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span>
<span class="hljs-comment">   *                   HBase Delete</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                                  tableName: <span class="hljs-type">String</span>,
                                  f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {
    dstream.foreach((rdd, time) =&gt; {
      bulkCheckDelete(rdd, tableName, f)
    })
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   *  Under lining function to support all bulk mutations</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   *  May be opened up if requested</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkMutation</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>, batchSize: <span class="hljs-type">Integer</span>) {
    rdd.foreachPartition(
      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](
        broadcastedConf,
        it,
        (iterator, hConnection) =&gt; {
          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)
          <span class="hljs-keyword">val</span> mutationList = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Mutation</span>]
          iterator.foreach(<span class="hljs-type">T</span> =&gt; {
            mutationList.add(f(<span class="hljs-type">T</span>))
            <span class="hljs-keyword">if</span> (mutationList.size &gt;= batchSize) {
              htable.batch(mutationList)
              mutationList.clear()
            }
          })
          <span class="hljs-keyword">if</span> (mutationList.size() &gt; <span class="hljs-number">0</span>) {
            htable.batch(mutationList)
            mutationList.clear()
          }
          htable.close()
        }))
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   *  Under lining function to support all bulk streaming mutations</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   *  May be opened up if requested</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkMutation</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                                    tableName: <span class="hljs-type">String</span>,
                                    f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>,
                                    batchSize: <span class="hljs-type">Integer</span>) = {
    dstream.foreach((rdd, time) =&gt; {
      bulkMutation(rdd, tableName, f, batchSize)
    })
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.mapPartition method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a RDD and generates a</span>
<span class="hljs-comment">   * new RDD based on Gets and the results they bring back from HBase</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param rdd     Original RDD with data to iterate over</span>
<span class="hljs-comment">   * @param tableName        The name of the table to get from</span>
<span class="hljs-comment">   * @param makeGet    function to convert a value in the RDD to a</span>
<span class="hljs-comment">   *                   HBase Get</span>
<span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span>
<span class="hljs-comment">   *                   what ever the user wants to put in the resulting</span>
<span class="hljs-comment">   *                   RDD</span>
<span class="hljs-comment">   * return            new RDD that is created by the Get to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](tableName: <span class="hljs-type">String</span>,
                    batchSize: <span class="hljs-type">Integer</span>,
                    rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],
                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,
                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {

    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,
      batchSize,
      makeGet,
      convertResult)

    rdd.mapPartitions[<span class="hljs-type">U</span>](it =&gt;
      hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](
        broadcastedConf,
        it,
        getMapPartition.run), <span class="hljs-literal">true</span>)(fakeClassTag[<span class="hljs-type">U</span>])
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMap method.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span>
<span class="hljs-comment">   * generates a new DStream based on Gets and the results</span>
<span class="hljs-comment">   * they bring back from HBase</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span>
<span class="hljs-comment">   * @param tableName The name of the table to get from</span>
<span class="hljs-comment">   * @param makeGet   function to convert a value in the DStream to a</span>
<span class="hljs-comment">   *                  HBase Get</span>
<span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span>
<span class="hljs-comment">   *                      what ever the user wants to put in the resulting</span>
<span class="hljs-comment">   *                      DStream</span>
<span class="hljs-comment">   * return            new DStream that is created by the Get to HBase</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>,
                                    batchSize: <span class="hljs-type">Integer</span>,
                                    dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],
                                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,
                                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {

    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,
      batchSize,
      makeGet,
      convertResult)

    dstream.mapPartitions[<span class="hljs-type">U</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](
      broadcastedConf,
      it,
      getMapPartition.run), <span class="hljs-literal">true</span>)
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * This function will use the native HBase TableInputFormat with the</span>
<span class="hljs-comment">   * given scan object to generate a new RDD</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   *  @param tableName the name of the table to scan</span>
<span class="hljs-comment">   *  @param scan      the HBase scan object to use to read data from HBase</span>
<span class="hljs-comment">   *  @param f         function to convert a Result object from HBase into</span>
<span class="hljs-comment">   *                   what the user wants in the final generated RDD</span>
<span class="hljs-comment">   *  @return          new RDD with results from scan</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>[<span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>, f: ((<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {

    <span class="hljs-keyword">var</span> job: <span class="hljs-type">Job</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(getConf(broadcastedConf))

    <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)
    <span class="hljs-type">TableMapReduceUtil</span>.initTableMapperJob(tableName, scan, classOf[<span class="hljs-type">IdentityTableMapper</span>], <span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, job)

    sc.newAPIHadoopRDD(job.getConfiguration(),
      classOf[<span class="hljs-type">TableInputFormat</span>],
      classOf[<span class="hljs-type">ImmutableBytesWritable</span>],
      classOf[<span class="hljs-type">Result</span>]).map(f)
  }


  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   * A overloaded version of HBaseContext hbaseRDD that predefines the</span>
<span class="hljs-comment">   * type of the outputing RDD</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   *  @param tableName the name of the table to scan</span>
<span class="hljs-comment">   *  @param scans      the HBase scan object to use to read data from HBase</span>
<span class="hljs-comment">   *  @return New RDD with results from scan</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>(tableName: <span class="hljs-type">String</span>, scans: <span class="hljs-type">Scan</span>):
  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {

    hbaseRDD[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])](
      tableName,
      scans,
      (r: (<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; {
        <span class="hljs-keyword">val</span> it = r._2.list().iterator()
        <span class="hljs-keyword">val</span> list = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])]()

        <span class="hljs-keyword">while</span> (it.hasNext()) {
          <span class="hljs-keyword">val</span> kv = it.next()
          list.add((kv.getFamily(), kv.getQualifier(), kv.getValue()))
        }

        (r._1.copyBytes(), list)
      })
  }

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseScanRDD</span></span>(tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>):
  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {

    <span class="hljs-keyword">new</span> <span class="hljs-type">HBaseScanRDD</span>(sc, tableName, scan,
      broadcastedConf)
  }


  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   *  Under lining wrapper all foreach functions in HBaseContext</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseForeachPartition</span></span>[<span class="hljs-type">T</span>](
                                        configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],
                                        it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>],
                                        f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {

    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)


    applyCreds(configBroadcast)
    <span class="hljs-comment">// specify that this is a proxy user</span>
    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)
    f(it, hConnection)
    hConnection.close()

  }



  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConf</span></span>(configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]): <span class="hljs-type">Configuration</span> = {

    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration != <span class="hljs-literal">null</span>) {
      tmpHdfsConfiguration
    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span>) {

      <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(<span class="hljs-type">SparkHadoopUtil</span>.get.conf)



      <span class="hljs-keyword">val</span> inputStream = fs.open(<span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile))
      tmpHdfsConfiguration = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>(<span class="hljs-literal">false</span>)
      tmpHdfsConfiguration.readFields(inputStream)
      inputStream.close()

      tmpHdfsConfiguration
    }

    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration == <span class="hljs-literal">null</span>) {
      <span class="hljs-keyword">try</span> {
        tmpHdfsConfiguration = configBroadcast.value.value
        tmpHdfsConfiguration
      } <span class="hljs-keyword">catch</span> {
        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt;{
          println(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)
        }
      }
    }


    tmpHdfsConfiguration
  }

  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   *  Under lining wrapper all mapPartition functions in HBaseContext</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseMapPartition</span></span>[<span class="hljs-type">K</span>, <span class="hljs-type">U</span>](
                                       configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],
                                       it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>],
                                       mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {

    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)
    applyCreds(configBroadcast)
    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)

    <span class="hljs-keyword">val</span> res = mp(it, hConnection)
    hConnection.close()
    res

  }


  <span class="hljs-comment">/**</span>
<span class="hljs-comment">   *  Under lining wrapper all get mapPartition functions in HBaseContext</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetMapPartition</span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<span class="hljs-params">tableName: <span class="hljs-type">String</span>,</span></span>
<span class="hljs-class"><span class="hljs-params">                                      batchSize: <span class="hljs-type">Integer</span>,</span></span>
<span class="hljs-class"><span class="hljs-params">                                      makeGet: (<span class="hljs-type">T</span></span>) <span class="hljs-title">=&gt;</span> <span class="hljs-title">Get</span>,</span>
<span class="hljs-class">                                      <span class="hljs-title">convertResult</span></span>: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>) <span class="hljs-keyword">extends</span> <span class="hljs-type">Serializable</span> {

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(iterator: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], hConnection: <span class="hljs-type">HConnection</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {
      <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)

      <span class="hljs-keyword">val</span> gets = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Get</span>]()
      <span class="hljs-keyword">var</span> res = <span class="hljs-type">List</span>[<span class="hljs-type">U</span>]()

      <span class="hljs-keyword">while</span> (iterator.hasNext) {
        gets.add(makeGet(iterator.next))

        <span class="hljs-keyword">if</span> (gets.size() == batchSize) {
          <span class="hljs-keyword">var</span> results = htable.get(gets)
          res = res ++ results.map(convertResult)
          gets.clear()
        }
      }
      <span class="hljs-keyword">if</span> (gets.size() &gt; <span class="hljs-number">0</span>) {
        <span class="hljs-keyword">val</span> results = htable.get(gets)
        res = res ++ results.map(convertResult)
        gets.clear()
      }
      htable.close()
      res.iterator
    }
  }



<span class="hljs-comment">/**</span>
<span class="hljs-comment">   * Produces a ClassTag[T], which is actually just a casted ClassTag[AnyRef].</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * This method is used to keep ClassTags out of the external Java API, as the Java compiler</span>
<span class="hljs-comment">   * cannot produce them automatically. While this ClassTag-faking does please the compiler,</span>
<span class="hljs-comment">   * it can cause problems at runtime if the Scala API relies on ClassTags for correctness.</span>
<span class="hljs-comment">   *</span>
<span class="hljs-comment">   * Often, though, a ClassTag[AnyRef] will not lead to incorrect behavior, just worse performance</span>
<span class="hljs-comment">   * or security issues. For instance, an Array[AnyRef] can hold any type T, but may lose primitive</span>
<span class="hljs-comment">   * specialization.</span>
<span class="hljs-comment">   */</span>
  <span class="hljs-keyword">private</span>[spark]
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fakeClassTag</span></span>[<span class="hljs-type">T</span>]: <span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>] = <span class="hljs-type">ClassTag</span>.<span class="hljs-type">AnyRef</span>.asInstanceOf[<span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>]]
}</code></span></span></span></pre></div>

<h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><p><em>优点</em>：</p>
<ol>
<li>使用方便，直接套用工具即可</li>
<li>不需要考虑不必要的序列化等问题</li>
<li>Cloudera出品，质量有保证</li>
</ol>
<p><em>缺点</em>：</p>
<ol>
<li>适合只写一张表的场景</li>
<li>应用场景收限，特别是需要自己保存Kafka的Offsets到Zookeeper时</li>
<li>定制化程度较高，不适合博主的需求</li>
</ol>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%B0%83%E7%A0%94/">调研</a>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                      <a class="hover-with-bg" href="/tags/Kafka/">Kafka</a>
                    
                      <a class="hover-with-bg" href="/tags/Hbase/">Hbase</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2018/06/29/Git-CI-CD%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%E3%80%90%E4%BA%8C%E3%80%91/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Git-CI/CD安装与使用【二】</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2018/06/29/Kafka-SparkStreaming-Hbase%E3%80%90%E4%BA%8C%E3%80%91/">
                        <span class="hidden-mobile">Kafka-&gt;SparkStreaming-&gt;Hbase【二】</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">×</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script>
<script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script>



<!-- Plugins -->


  
    
  









  <script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script>
  



  <script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script>
  



  <script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script>
  



  
  



  <script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  

  






















<script src="/bundle.js"></script><script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  ;

    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Kafka->SparkStreaming->Hbase【一】&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  ;

    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  ;

    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  ;

    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script></body></html>