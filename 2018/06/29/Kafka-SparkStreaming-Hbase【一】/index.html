<!DOCTYPE html><html lang="zh-CN"><head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="xinshiyou">
  <meta name="keywords" content="">
  <title>Kafka-&gt;SparkStreaming-&gt;Hbase【一】 - 星空捞月：找寻心中的安宁</title>

  


  
  

  
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->










<!-- 自定义样式保持在最底部 -->


  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星空捞月：找寻心中的安宁" type="application/atom+xml">
<script>function loadCss(l){var d=document,h=d.head,s=d.createElement('link');s.rel='stylesheet';s.href=l;!function e(f){if (d.body)return f();setTimeout(function(){e(f)})}(function(){h.appendChild(s);});}loadCss('/style.css');loadCss('https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css');loadCss('https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css');loadCss('https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css');loadCss('//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css');loadCss('//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css');loadCss('https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css');</script><noscript><link rel="stylesheet" href="/style.css"><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"></noscript></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>星空捞月：找寻心中的安宁</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax="true" style="background: url('/img/top.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2018-06-29 16:52">
      2018年6月29日 下午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      118
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>  根据业务需求，将Kafka中数据抽取插入到Hbase中。目前网上可以找到许多相关的文章，这里介绍Github上的一个开源工具。</p>
<a id="more"></a>

<h2 id="工具地址"><a href="#工具地址" class="headerlink" title="工具地址"></a>工具地址</h2><h3 id="Github上搜索结果"><a href="#Github上搜索结果" class="headerlink" title="Github上搜索结果"></a>Github上搜索结果</h3><p><img src="001.png" srcset="/img/loading.gif" alt="这里写图片描述"></p>
<h3 id="选择工具"><a href="#选择工具" class="headerlink" title="选择工具"></a>选择工具</h3><p><a href="https://github.com/cloudera-labs/SparkOnHBase" target="_blank" rel="noopener">SparkOnHbase</a></p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><ol>
<li>Hadoop: 2.6.0-cdh5.12.1</li>
<li>Spark:   1.6.0-2.10.5</li>
<li>Hbase:  1.2.0-cdh5.12.1</li>
<li>Hive:     1.1.0-cdh5.12.1</li>
<li>Kafka:   kafka_2.10-0.9.0</li>
<li>OS:       CentOS Linux release 7.4.1708 (Core)</li>
<li>JDK:      jdk1.8.0_151</li>
</ol>
<h2 id="场景需求"><a href="#场景需求" class="headerlink" title="场景需求"></a>场景需求</h2><ol>
<li>支持自定义接口解析Kafka对象</li>
<li>支持插入不同的Hbase表，即配置多个Hbase表名</li>
<li>Kafka偏移量，可以写入Zookeeper，自定义偏移量</li>
</ol>
<h2 id="工具研究"><a href="#工具研究" class="headerlink" title="工具研究"></a>工具研究</h2><h3 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h3><p>  对于Scala用户，主要是以下源代码</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><div class="hljs"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br></pre></div></td><td class="code"><div class="hljs"><pre class=" language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params">@transient sc: <span class="hljs-type">SparkContext</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                   @transient config: <span class="hljs-type">Configuration</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                    val tmpHdfsConfgFile: <span class="hljs-type">String</span> = null</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Logging</span> </span>{<br><br><br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> tmpHdfsConfiguration:<span class="hljs-type">Configuration</span> = config<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> appliedCredentials = <span class="hljs-literal">false</span>;<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> job = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(config)<br>  <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>  <span class="hljs-keyword">val</span> broadcastedConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(config))<br>  <span class="hljs-keyword">val</span> credentialsConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(job.getCredentials()))<br><br>  <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span> &amp;&amp; config != <span class="hljs-literal">null</span>) {<br>    <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(config)<br>    <span class="hljs-keyword">val</span> tmpPath = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile)<br>    <span class="hljs-keyword">if</span> (!fs.exists(tmpPath)) {<br>      <span class="hljs-keyword">val</span> outputStream = fs.create(tmpPath)<br>      config.write(outputStream)<br>      outputStream.close();<br>    } <span class="hljs-keyword">else</span> {<br>      logWarning(<span class="hljs-string">"tmpHdfsConfigDir "</span> + tmpHdfsConfgFile + <span class="hljs-string">" exist!!"</span>)<br>    }<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD foreachPartition.</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param f    Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span><br><span class="hljs-comment">   *             with HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachPartition</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                          f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition(broadcastedConf, it, f))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming dStream foreach</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param f        Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *                 the DStream values and a HConnection object to</span><br><span class="hljs-comment">   *                 interact with HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachRDD</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                    f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      foreachPartition(rdd, f)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD mapPartition.</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span><br><span class="hljs-comment">   *       getting data from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param mp   Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span><br><span class="hljs-comment">   *             with HBase</span><br><span class="hljs-comment">   * @return     Returns a new RDD generated by the user definition</span><br><span class="hljs-comment">   *             function just like normal mapPartition</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mapPartition</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>: <span class="hljs-type">ClassTag</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                                   mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">R</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">R</span>] = {<br><br>    rdd.mapPartitions[<span class="hljs-type">R</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>](broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming DStream</span><br><span class="hljs-comment">   * mapPartition.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span><br><span class="hljs-comment">   *       getting data from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param mp       Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *                 the DStream values and a HConnection object to</span><br><span class="hljs-comment">   *                 interact with HBase</span><br><span class="hljs-comment">   * @return         Returns a new DStream generated by the user</span><br><span class="hljs-comment">   *                 definition function just like normal mapPartition</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamMap</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {<br><br>    dstream.mapPartitions(it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>      broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take RDD</span><br><span class="hljs-comment">   * and generate puts and send them to HBase.</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to put into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a HBase Put</span><br><span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>, autoFlush: <span class="hljs-type">Boolean</span>) {<br><br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; htable.put(f(<span class="hljs-type">T</span>)))<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">applyCreds</span></span>[<span class="hljs-type">T</span>] (configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]){<br><br><br>    credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br><br>    logInfo(<span class="hljs-string">"appliedCredentials:"</span> + appliedCredentials + <span class="hljs-string">",credentials:"</span> + credentials);<br><br>    <span class="hljs-keyword">if</span> (appliedCredentials == <span class="hljs-literal">false</span> &amp;&amp; credentials != <span class="hljs-literal">null</span>) {<br>      appliedCredentials = <span class="hljs-literal">true</span><br>      logCredInformation(credentials)<br><br>      <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> ugi = <span class="hljs-type">UserGroupInformation</span>.getCurrentUser();<br>      ugi.addCredentials(credentials)<br>      <span class="hljs-comment">// specify that this is a proxy user</span><br>      ugi.setAuthenticationMethod(<span class="hljs-type">AuthenticationMethod</span>.<span class="hljs-type">PROXY</span>)<br><br>      ugi.addCredentials(credentialsConf.value.value)<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logCredInformation</span></span>[<span class="hljs-type">T</span>] (credentials2:<span class="hljs-type">Credentials</span>) {<br>    logInfo(<span class="hljs-string">"credentials:"</span> + credentials2);<br>    <span class="hljs-keyword">for</span> (a &lt;- <span class="hljs-number">0</span> until credentials2.getAllSecretKeys.size()) {<br>      logInfo(<span class="hljs-string">"getAllSecretKeys:"</span> + a + <span class="hljs-string">":"</span> + credentials2.getAllSecretKeys.get(a));<br>    }<br>    <span class="hljs-keyword">val</span> it = credentials2.getAllTokens.iterator();<br>    <span class="hljs-keyword">while</span> (it.hasNext) {<br>      logInfo(<span class="hljs-string">"getAllTokens:"</span> + it.next());<br>    }<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate puts and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to put into</span><br><span class="hljs-comment">   * @param f          Function to convert a value in</span><br><span class="hljs-comment">   *                   the DStream to a HBase Put</span><br><span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                       tableName: <span class="hljs-type">String</span>,<br>                       f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>,<br>                       autoFlush: <span class="hljs-type">Boolean</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkPut(rdd, tableName, f, autoFlush)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take RDD</span><br><span class="hljs-comment">   * and generate checkAndPuts and send them to HBase.</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to put into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to</span><br><span class="hljs-comment">   *                  a HBase checkAndPut</span><br><span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br><br><br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br><br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            <span class="hljs-keyword">val</span> checkPut = f(<span class="hljs-type">T</span>)<br>            htable.checkAndPut(checkPut._1, checkPut._2, checkPut._3, checkPut._4, checkPut._5)<br>          })<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate checkAndPuts and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to checkAndPut into</span><br><span class="hljs-comment">   * @param f          function to convert a value in the RDD to</span><br><span class="hljs-comment">   *                   a HBase checkAndPut</span><br><span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkCheckAndPut(rdd, tableName, f, autoFlush)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and</span><br><span class="hljs-comment">   * generate increments and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to increment to</span><br><span class="hljs-comment">   * @param f         function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Increments</span><br><span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkIncrement</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    bulkMutation(rdd, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate delete</span><br><span class="hljs-comment">   * and send them to HBase.  The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to delete from</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Deletes</span><br><span class="hljs-comment">   * @param batchSize       The number of delete to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    bulkMutation(rdd, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate</span><br><span class="hljs-comment">   * checkAndDelete and send them to HBase.  The complexity of managing the</span><br><span class="hljs-comment">   * HConnection is removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to delete from</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Deletes</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                         tableName: <span class="hljs-type">String</span>,<br>                         f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            <span class="hljs-keyword">val</span> checkDelete = f(<span class="hljs-type">T</span>)<br>            htable.checkAndDelete(checkDelete._1, checkDelete._2, checkDelete._3, checkDelete._4, checkDelete._5)<br>          })<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate Increments and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to increments into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                  HBase Increments</span><br><span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkIncrement</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                             tableName: <span class="hljs-type">String</span>,<br>                             f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>,<br>                             batchSize: <span class="hljs-type">Int</span>) = {<br>    streamBulkMutation(dstream, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate Delete and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to delete from</span><br><span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                   HBase Delete</span><br><span class="hljs-comment">   * @param batchSize        The number of deletes to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                          tableName: <span class="hljs-type">String</span>,<br>                          f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>,<br>                          batchSize: <span class="hljs-type">Integer</span>) = {<br>    streamBulkMutation(dstream, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the bulkCheckDelete method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate CheckAndDelete and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to delete from</span><br><span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                   HBase Delete</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                  tableName: <span class="hljs-type">String</span>,<br>                                  f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkCheckDelete(rdd, tableName, f)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining function to support all bulk mutations</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  May be opened up if requested</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkMutation</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          <span class="hljs-keyword">val</span> mutationList = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Mutation</span>]<br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            mutationList.add(f(<span class="hljs-type">T</span>))<br>            <span class="hljs-keyword">if</span> (mutationList.size &gt;= batchSize) {<br>              htable.batch(mutationList)<br>              mutationList.clear()<br>            }<br>          })<br>          <span class="hljs-keyword">if</span> (mutationList.size() &gt; <span class="hljs-number">0</span>) {<br>            htable.batch(mutationList)<br>            mutationList.clear()<br>          }<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining function to support all bulk streaming mutations</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  May be opened up if requested</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkMutation</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                    tableName: <span class="hljs-type">String</span>,<br>                                    f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>,<br>                                    batchSize: <span class="hljs-type">Integer</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkMutation(rdd, tableName, f, batchSize)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.mapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generates a</span><br><span class="hljs-comment">   * new RDD based on Gets and the results they bring back from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd     Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName        The name of the table to get from</span><br><span class="hljs-comment">   * @param makeGet    function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                   HBase Get</span><br><span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span><br><span class="hljs-comment">   *                   what ever the user wants to put in the resulting</span><br><span class="hljs-comment">   *                   RDD</span><br><span class="hljs-comment">   * return            new RDD that is created by the Get to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](tableName: <span class="hljs-type">String</span>,<br>                    batchSize: <span class="hljs-type">Integer</span>,<br>                    rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,<br>                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,<br>      batchSize,<br>      makeGet,<br>      convertResult)<br><br>    rdd.mapPartitions[<span class="hljs-type">U</span>](it =&gt;<br>      hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>        broadcastedConf,<br>        it,<br>        getMapPartition.run), <span class="hljs-literal">true</span>)(fakeClassTag[<span class="hljs-type">U</span>])<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMap method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generates a new DStream based on Gets and the results</span><br><span class="hljs-comment">   * they bring back from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to get from</span><br><span class="hljs-comment">   * @param makeGet   function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                  HBase Get</span><br><span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span><br><span class="hljs-comment">   *                      what ever the user wants to put in the resulting</span><br><span class="hljs-comment">   *                      DStream</span><br><span class="hljs-comment">   * return            new DStream that is created by the Get to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>,<br>                                    batchSize: <span class="hljs-type">Integer</span>,<br>                                    dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,<br>                                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,<br>      batchSize,<br>      makeGet,<br>      convertResult)<br><br>    dstream.mapPartitions[<span class="hljs-type">U</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>      broadcastedConf,<br>      it,<br>      getMapPartition.run), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * This function will use the native HBase TableInputFormat with the</span><br><span class="hljs-comment">   * given scan object to generate a new RDD</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  @param tableName the name of the table to scan</span><br><span class="hljs-comment">   *  @param scan      the HBase scan object to use to read data from HBase</span><br><span class="hljs-comment">   *  @param f         function to convert a Result object from HBase into</span><br><span class="hljs-comment">   *                   what the user wants in the final generated RDD</span><br><span class="hljs-comment">   *  @return          new RDD with results from scan</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>[<span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>, f: ((<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">var</span> job: <span class="hljs-type">Job</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(getConf(broadcastedConf))<br><br>    <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>    <span class="hljs-type">TableMapReduceUtil</span>.initTableMapperJob(tableName, scan, classOf[<span class="hljs-type">IdentityTableMapper</span>], <span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, job)<br><br>    sc.newAPIHadoopRDD(job.getConfiguration(),<br>      classOf[<span class="hljs-type">TableInputFormat</span>],<br>      classOf[<span class="hljs-type">ImmutableBytesWritable</span>],<br>      classOf[<span class="hljs-type">Result</span>]).map(f)<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A overloaded version of HBaseContext hbaseRDD that predefines the</span><br><span class="hljs-comment">   * type of the outputing RDD</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  @param tableName the name of the table to scan</span><br><span class="hljs-comment">   *  @param scans      the HBase scan object to use to read data from HBase</span><br><span class="hljs-comment">   *  @return New RDD with results from scan</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>(tableName: <span class="hljs-type">String</span>, scans: <span class="hljs-type">Scan</span>):<br>  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    hbaseRDD[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])](<br>      tableName,<br>      scans,<br>      (r: (<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; {<br>        <span class="hljs-keyword">val</span> it = r._2.list().iterator()<br>        <span class="hljs-keyword">val</span> list = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])]()<br><br>        <span class="hljs-keyword">while</span> (it.hasNext()) {<br>          <span class="hljs-keyword">val</span> kv = it.next()<br>          list.add((kv.getFamily(), kv.getQualifier(), kv.getValue()))<br>        }<br><br>        (r._1.copyBytes(), list)<br>      })<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseScanRDD</span></span>(tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>):<br>  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    <span class="hljs-keyword">new</span> <span class="hljs-type">HBaseScanRDD</span>(sc, tableName, scan,<br>      broadcastedConf)<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all foreach functions in HBaseContext</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseForeachPartition</span></span>[<span class="hljs-type">T</span>](<br>                                        configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>                                        it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>],<br>                                        f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br><br><br>    applyCreds(configBroadcast)<br>    <span class="hljs-comment">// specify that this is a proxy user</span><br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    f(it, hConnection)<br>    hConnection.close()<br><br>  }<br><br><br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConf</span></span>(configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]): <span class="hljs-type">Configuration</span> = {<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration != <span class="hljs-literal">null</span>) {<br>      tmpHdfsConfiguration<br>    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span>) {<br><br>      <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(<span class="hljs-type">SparkHadoopUtil</span>.get.conf)<br><br><br><br>      <span class="hljs-keyword">val</span> inputStream = fs.open(<span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile))<br>      tmpHdfsConfiguration = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>(<span class="hljs-literal">false</span>)<br>      tmpHdfsConfiguration.readFields(inputStream)<br>      inputStream.close()<br><br>      tmpHdfsConfiguration<br>    }<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration == <span class="hljs-literal">null</span>) {<br>      <span class="hljs-keyword">try</span> {<br>        tmpHdfsConfiguration = configBroadcast.value.value<br>        tmpHdfsConfiguration<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt;{<br>          println(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br><br><br>    tmpHdfsConfiguration<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all mapPartition functions in HBaseContext</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseMapPartition</span></span>[<span class="hljs-type">K</span>, <span class="hljs-type">U</span>](<br>                                       configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>                                       it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>],<br>                                       mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br><br>    <span class="hljs-keyword">val</span> res = mp(it, hConnection)<br>    hConnection.close()<br>    res<br><br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all get mapPartition functions in HBaseContext</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetMapPartition</span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<span class="hljs-params">tableName: <span class="hljs-type">String</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                                      batchSize: <span class="hljs-type">Integer</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                                      makeGet: (<span class="hljs-type">T</span></span>) <span class="hljs-title">=&gt;</span> <span class="hljs-title">Get</span>,</span><br><span class="hljs-class">                                      <span class="hljs-title">convertResult</span></span>: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>) <span class="hljs-keyword">extends</span> <span class="hljs-type">Serializable</span> {<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(iterator: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], hConnection: <span class="hljs-type">HConnection</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br>      <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>      <span class="hljs-keyword">val</span> gets = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Get</span>]()<br>      <span class="hljs-keyword">var</span> res = <span class="hljs-type">List</span>[<span class="hljs-type">U</span>]()<br><br>      <span class="hljs-keyword">while</span> (iterator.hasNext) {<br>        gets.add(makeGet(iterator.next))<br><br>        <span class="hljs-keyword">if</span> (gets.size() == batchSize) {<br>          <span class="hljs-keyword">var</span> results = htable.get(gets)<br>          res = res ++ results.map(convertResult)<br>          gets.clear()<br>        }<br>      }<br>      <span class="hljs-keyword">if</span> (gets.size() &gt; <span class="hljs-number">0</span>) {<br>        <span class="hljs-keyword">val</span> results = htable.get(gets)<br>        res = res ++ results.map(convertResult)<br>        gets.clear()<br>      }<br>      htable.close()<br>      res.iterator<br>    }<br>  }<br><br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">   * Produces a ClassTag[T], which is actually just a casted ClassTag[AnyRef].</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * This method is used to keep ClassTags out of the external Java API, as the Java compiler</span><br><span class="hljs-comment">   * cannot produce them automatically. While this ClassTag-faking does please the compiler,</span><br><span class="hljs-comment">   * it can cause problems at runtime if the Scala API relies on ClassTags for correctness.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Often, though, a ClassTag[AnyRef] will not lead to incorrect behavior, just worse performance</span><br><span class="hljs-comment">   * or security issues. For instance, an Array[AnyRef] can hold any type T, but may lose primitive</span><br><span class="hljs-comment">   * specialization.</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span>[spark]<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fakeClassTag</span></span>[<span class="hljs-type">T</span>]: <span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>] = <span class="hljs-type">ClassTag</span>.<span class="hljs-type">AnyRef</span>.asInstanceOf[<span class="hljs-type">ClassTag</span>[<span class="hljs-type"><code class="language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params">@transient sc: <span class="hljs-type">SparkContext</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                   @transient config: <span class="hljs-type">Configuration</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                    val tmpHdfsConfgFile: <span class="hljs-type">String</span> = null</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Logging</span> </span>{<br><br><br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> tmpHdfsConfiguration:<span class="hljs-type">Configuration</span> = config<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> appliedCredentials = <span class="hljs-literal">false</span>;<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> job = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(config)<br>  <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>  <span class="hljs-keyword">val</span> broadcastedConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(config))<br>  <span class="hljs-keyword">val</span> credentialsConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(job.getCredentials()))<br><br>  <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span> &amp;&amp; config != <span class="hljs-literal">null</span>) {<br>    <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(config)<br>    <span class="hljs-keyword">val</span> tmpPath = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile)<br>    <span class="hljs-keyword">if</span> (!fs.exists(tmpPath)) {<br>      <span class="hljs-keyword">val</span> outputStream = fs.create(tmpPath)<br>      config.write(outputStream)<br>      outputStream.close();<br>    } <span class="hljs-keyword">else</span> {<br>      logWarning(<span class="hljs-string">"tmpHdfsConfigDir "</span> + tmpHdfsConfgFile + <span class="hljs-string">" exist!!"</span>)<br>    }<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD foreachPartition.</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param f    Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span><br><span class="hljs-comment">   *             with HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachPartition</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                          f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition(broadcastedConf, it, f))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming dStream foreach</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param f        Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *                 the DStream values and a HConnection object to</span><br><span class="hljs-comment">   *                 interact with HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">foreachRDD</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                    f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      foreachPartition(rdd, f)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark RDD mapPartition.</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span><br><span class="hljs-comment">   *       getting data from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd  Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param mp   Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *             the RDD values and a HConnection object to interact</span><br><span class="hljs-comment">   *             with HBase</span><br><span class="hljs-comment">   * @return     Returns a new RDD generated by the user definition</span><br><span class="hljs-comment">   *             function just like normal mapPartition</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mapPartition</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>: <span class="hljs-type">ClassTag</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                                   mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">R</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">R</span>] = {<br><br>    rdd.mapPartitions[<span class="hljs-type">R</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>](broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple enrichment of the traditional Spark Streaming DStream</span><br><span class="hljs-comment">   * mapPartition.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * This function differs from the original in that it offers the</span><br><span class="hljs-comment">   * developer access to a already connected HConnection object</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Do not close the HConnection object.  All HConnection</span><br><span class="hljs-comment">   * management is handled outside this method</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Note: Make sure to partition correctly to avoid memory issue when</span><br><span class="hljs-comment">   *       getting data from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream  Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param mp       Function to be given a iterator to iterate through</span><br><span class="hljs-comment">   *                 the DStream values and a HConnection object to</span><br><span class="hljs-comment">   *                 interact with HBase</span><br><span class="hljs-comment">   * @return         Returns a new DStream generated by the user</span><br><span class="hljs-comment">   *                 definition function just like normal mapPartition</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamMap</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {<br><br>    dstream.mapPartitions(it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>      broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take RDD</span><br><span class="hljs-comment">   * and generate puts and send them to HBase.</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to put into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a HBase Put</span><br><span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>, autoFlush: <span class="hljs-type">Boolean</span>) {<br><br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; htable.put(f(<span class="hljs-type">T</span>)))<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">applyCreds</span></span>[<span class="hljs-type">T</span>] (configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]){<br><br><br>    credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br><br>    logInfo(<span class="hljs-string">"appliedCredentials:"</span> + appliedCredentials + <span class="hljs-string">",credentials:"</span> + credentials);<br><br>    <span class="hljs-keyword">if</span> (appliedCredentials == <span class="hljs-literal">false</span> &amp;&amp; credentials != <span class="hljs-literal">null</span>) {<br>      appliedCredentials = <span class="hljs-literal">true</span><br>      logCredInformation(credentials)<br><br>      <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> ugi = <span class="hljs-type">UserGroupInformation</span>.getCurrentUser();<br>      ugi.addCredentials(credentials)<br>      <span class="hljs-comment">// specify that this is a proxy user</span><br>      ugi.setAuthenticationMethod(<span class="hljs-type">AuthenticationMethod</span>.<span class="hljs-type">PROXY</span>)<br><br>      ugi.addCredentials(credentialsConf.value.value)<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logCredInformation</span></span>[<span class="hljs-type">T</span>] (credentials2:<span class="hljs-type">Credentials</span>) {<br>    logInfo(<span class="hljs-string">"credentials:"</span> + credentials2);<br>    <span class="hljs-keyword">for</span> (a &lt;- <span class="hljs-number">0</span> until credentials2.getAllSecretKeys.size()) {<br>      logInfo(<span class="hljs-string">"getAllSecretKeys:"</span> + a + <span class="hljs-string">":"</span> + credentials2.getAllSecretKeys.get(a));<br>    }<br>    <span class="hljs-keyword">val</span> it = credentials2.getAllTokens.iterator();<br>    <span class="hljs-keyword">while</span> (it.hasNext) {<br>      logInfo(<span class="hljs-string">"getAllTokens:"</span> + it.next());<br>    }<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate puts and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to put into</span><br><span class="hljs-comment">   * @param f          Function to convert a value in</span><br><span class="hljs-comment">   *                   the DStream to a HBase Put</span><br><span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                       tableName: <span class="hljs-type">String</span>,<br>                       f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Put</span>,<br>                       autoFlush: <span class="hljs-type">Boolean</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkPut(rdd, tableName, f, autoFlush)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take RDD</span><br><span class="hljs-comment">   * and generate checkAndPuts and send them to HBase.</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to put into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to</span><br><span class="hljs-comment">   *                  a HBase checkAndPut</span><br><span class="hljs-comment">   * @param autoFlush If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br><br><br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br><br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            <span class="hljs-keyword">val</span> checkPut = f(<span class="hljs-type">T</span>)<br>            htable.checkAndPut(checkPut._1, checkPut._2, checkPut._3, checkPut._4, checkPut._5)<br>          })<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate checkAndPuts and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to checkAndPut into</span><br><span class="hljs-comment">   * @param f          function to convert a value in the RDD to</span><br><span class="hljs-comment">   *                   a HBase checkAndPut</span><br><span class="hljs-comment">   * @param autoFlush        If autoFlush should be turned on</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndPut</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Put</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkCheckAndPut(rdd, tableName, f, autoFlush)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and</span><br><span class="hljs-comment">   * generate increments and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to increment to</span><br><span class="hljs-comment">   * @param f         function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Increments</span><br><span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkIncrement</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    bulkMutation(rdd, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate delete</span><br><span class="hljs-comment">   * and send them to HBase.  The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to delete from</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Deletes</span><br><span class="hljs-comment">   * @param batchSize       The number of delete to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    bulkMutation(rdd, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.foreachPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generate</span><br><span class="hljs-comment">   * checkAndDelete and send them to HBase.  The complexity of managing the</span><br><span class="hljs-comment">   * HConnection is removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd       Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to delete from</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                  HBase Deletes</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkCheckDelete</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                         tableName: <span class="hljs-type">String</span>,<br>                         f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            <span class="hljs-keyword">val</span> checkDelete = f(<span class="hljs-type">T</span>)<br>            htable.checkAndDelete(checkDelete._1, checkDelete._2, checkDelete._3, checkDelete._4, checkDelete._5)<br>          })<br>          htable.flushCommits()<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate Increments and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to increments into</span><br><span class="hljs-comment">   * @param f         Function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                  HBase Increments</span><br><span class="hljs-comment">   * @param batchSize       The number of increments to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkIncrement</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                             tableName: <span class="hljs-type">String</span>,<br>                             f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Increment</span>,<br>                             batchSize: <span class="hljs-type">Int</span>) = {<br>    streamBulkMutation(dstream, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamBulkMutation method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate Delete and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to delete from</span><br><span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                   HBase Delete</span><br><span class="hljs-comment">   * @param batchSize        The number of deletes to batch before sending to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                          tableName: <span class="hljs-type">String</span>,<br>                          f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Delete</span>,<br>                          batchSize: <span class="hljs-type">Integer</span>) = {<br>    streamBulkMutation(dstream, tableName, f, batchSize)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the bulkCheckDelete method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generate CheckAndDelete and send them to HBase.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * The complexity of managing the HConnection is</span><br><span class="hljs-comment">   * removed from the developer</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream    Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName  The name of the table to delete from</span><br><span class="hljs-comment">   * @param f          function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                   HBase Delete</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkCheckAndDelete</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                  tableName: <span class="hljs-type">String</span>,<br>                                  f: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Delete</span>)) {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkCheckDelete(rdd, tableName, f)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining function to support all bulk mutations</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  May be opened up if requested</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkMutation</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], tableName: <span class="hljs-type">String</span>, f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>, batchSize: <span class="hljs-type">Integer</span>) {<br>    rdd.foreachPartition(<br>      it =&gt; hbaseForeachPartition[<span class="hljs-type">T</span>](<br>        broadcastedConf,<br>        it,<br>        (iterator, hConnection) =&gt; {<br>          <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br>          <span class="hljs-keyword">val</span> mutationList = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Mutation</span>]<br>          iterator.foreach(<span class="hljs-type">T</span> =&gt; {<br>            mutationList.add(f(<span class="hljs-type">T</span>))<br>            <span class="hljs-keyword">if</span> (mutationList.size &gt;= batchSize) {<br>              htable.batch(mutationList)<br>              mutationList.clear()<br>            }<br>          })<br>          <span class="hljs-keyword">if</span> (mutationList.size() &gt; <span class="hljs-number">0</span>) {<br>            htable.batch(mutationList)<br>            mutationList.clear()<br>          }<br>          htable.close()<br>        }))<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining function to support all bulk streaming mutations</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  May be opened up if requested</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkMutation</span></span>[<span class="hljs-type">T</span>](dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                    tableName: <span class="hljs-type">String</span>,<br>                                    f: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Mutation</span>,<br>                                    batchSize: <span class="hljs-type">Integer</span>) = {<br>    dstream.foreach((rdd, time) =&gt; {<br>      bulkMutation(rdd, tableName, f, batchSize)<br>    })<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.mapPartition method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a RDD and generates a</span><br><span class="hljs-comment">   * new RDD based on Gets and the results they bring back from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param rdd     Original RDD with data to iterate over</span><br><span class="hljs-comment">   * @param tableName        The name of the table to get from</span><br><span class="hljs-comment">   * @param makeGet    function to convert a value in the RDD to a</span><br><span class="hljs-comment">   *                   HBase Get</span><br><span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span><br><span class="hljs-comment">   *                   what ever the user wants to put in the resulting</span><br><span class="hljs-comment">   *                   RDD</span><br><span class="hljs-comment">   * return            new RDD that is created by the Get to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](tableName: <span class="hljs-type">String</span>,<br>                    batchSize: <span class="hljs-type">Integer</span>,<br>                    rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,<br>                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,<br>      batchSize,<br>      makeGet,<br>      convertResult)<br><br>    rdd.mapPartitions[<span class="hljs-type">U</span>](it =&gt;<br>      hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>        broadcastedConf,<br>        it,<br>        getMapPartition.run), <span class="hljs-literal">true</span>)(fakeClassTag[<span class="hljs-type">U</span>])<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A simple abstraction over the HBaseContext.streamMap method.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * It allow addition support for a user to take a DStream and</span><br><span class="hljs-comment">   * generates a new DStream based on Gets and the results</span><br><span class="hljs-comment">   * they bring back from HBase</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * @param dstream   Original DStream with data to iterate over</span><br><span class="hljs-comment">   * @param tableName The name of the table to get from</span><br><span class="hljs-comment">   * @param makeGet   function to convert a value in the DStream to a</span><br><span class="hljs-comment">   *                  HBase Get</span><br><span class="hljs-comment">   * @param convertResult This will convert the HBase Result object to</span><br><span class="hljs-comment">   *                      what ever the user wants to put in the resulting</span><br><span class="hljs-comment">   *                      DStream</span><br><span class="hljs-comment">   * return            new DStream that is created by the Get to HBase</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">streamBulkGet</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>,<br>                                    batchSize: <span class="hljs-type">Integer</span>,<br>                                    dstream: <span class="hljs-type">DStream</span>[<span class="hljs-type">T</span>],<br>                                    makeGet: (<span class="hljs-type">T</span>) =&gt; <span class="hljs-type">Get</span>,<br>                                    convertResult: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">DStream</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> getMapPartition = <span class="hljs-keyword">new</span> <span class="hljs-type">GetMapPartition</span>(tableName,<br>      batchSize,<br>      makeGet,<br>      convertResult)<br><br>    dstream.mapPartitions[<span class="hljs-type">U</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<br>      broadcastedConf,<br>      it,<br>      getMapPartition.run), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * This function will use the native HBase TableInputFormat with the</span><br><span class="hljs-comment">   * given scan object to generate a new RDD</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  @param tableName the name of the table to scan</span><br><span class="hljs-comment">   *  @param scan      the HBase scan object to use to read data from HBase</span><br><span class="hljs-comment">   *  @param f         function to convert a Result object from HBase into</span><br><span class="hljs-comment">   *                   what the user wants in the final generated RDD</span><br><span class="hljs-comment">   *  @return          new RDD with results from scan</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>[<span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>, f: ((<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">var</span> job: <span class="hljs-type">Job</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(getConf(broadcastedConf))<br><br>    <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>    <span class="hljs-type">TableMapReduceUtil</span>.initTableMapperJob(tableName, scan, classOf[<span class="hljs-type">IdentityTableMapper</span>], <span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, job)<br><br>    sc.newAPIHadoopRDD(job.getConfiguration(),<br>      classOf[<span class="hljs-type">TableInputFormat</span>],<br>      classOf[<span class="hljs-type">ImmutableBytesWritable</span>],<br>      classOf[<span class="hljs-type">Result</span>]).map(f)<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * A overloaded version of HBaseContext hbaseRDD that predefines the</span><br><span class="hljs-comment">   * type of the outputing RDD</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   *  @param tableName the name of the table to scan</span><br><span class="hljs-comment">   *  @param scans      the HBase scan object to use to read data from HBase</span><br><span class="hljs-comment">   *  @return New RDD with results from scan</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>(tableName: <span class="hljs-type">String</span>, scans: <span class="hljs-type">Scan</span>):<br>  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    hbaseRDD[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])](<br>      tableName,<br>      scans,<br>      (r: (<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; {<br>        <span class="hljs-keyword">val</span> it = r._2.list().iterator()<br>        <span class="hljs-keyword">val</span> list = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])]()<br><br>        <span class="hljs-keyword">while</span> (it.hasNext()) {<br>          <span class="hljs-keyword">val</span> kv = it.next()<br>          list.add((kv.getFamily(), kv.getQualifier(), kv.getValue()))<br>        }<br><br>        (r._1.copyBytes(), list)<br>      })<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseScanRDD</span></span>(tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>):<br>  <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    <span class="hljs-keyword">new</span> <span class="hljs-type">HBaseScanRDD</span>(sc, tableName, scan,<br>      broadcastedConf)<br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all foreach functions in HBaseContext</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseForeachPartition</span></span>[<span class="hljs-type">T</span>](<br>                                        configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>                                        it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>],<br>                                        f: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Unit</span>) = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br><br><br>    applyCreds(configBroadcast)<br>    <span class="hljs-comment">// specify that this is a proxy user</span><br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    f(it, hConnection)<br>    hConnection.close()<br><br>  }<br><br><br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConf</span></span>(configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]): <span class="hljs-type">Configuration</span> = {<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration != <span class="hljs-literal">null</span>) {<br>      tmpHdfsConfiguration<br>    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span>) {<br><br>      <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(<span class="hljs-type">SparkHadoopUtil</span>.get.conf)<br><br><br><br>      <span class="hljs-keyword">val</span> inputStream = fs.open(<span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile))<br>      tmpHdfsConfiguration = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>(<span class="hljs-literal">false</span>)<br>      tmpHdfsConfiguration.readFields(inputStream)<br>      inputStream.close()<br><br>      tmpHdfsConfiguration<br>    }<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration == <span class="hljs-literal">null</span>) {<br>      <span class="hljs-keyword">try</span> {<br>        tmpHdfsConfiguration = configBroadcast.value.value<br>        tmpHdfsConfiguration<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt;{<br>          println(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br><br><br>    tmpHdfsConfiguration<br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all mapPartition functions in HBaseContext</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseMapPartition</span></span>[<span class="hljs-type">K</span>, <span class="hljs-type">U</span>](<br>                                       configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>                                       it: <span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>],<br>                                       mp: (<span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br><br>    <span class="hljs-keyword">val</span> res = mp(it, hConnection)<br>    hConnection.close()<br>    res<br><br>  }<br><br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   *  Under lining wrapper all get mapPartition functions in HBaseContext</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetMapPartition</span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<span class="hljs-params">tableName: <span class="hljs-type">String</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                                      batchSize: <span class="hljs-type">Integer</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">                                      makeGet: (<span class="hljs-type">T</span></span>) <span class="hljs-title">=&gt;</span> <span class="hljs-title">Get</span>,</span><br><span class="hljs-class">                                      <span class="hljs-title">convertResult</span></span>: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>) <span class="hljs-keyword">extends</span> <span class="hljs-type">Serializable</span> {<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(iterator: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], hConnection: <span class="hljs-type">HConnection</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br>      <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>      <span class="hljs-keyword">val</span> gets = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Get</span>]()<br>      <span class="hljs-keyword">var</span> res = <span class="hljs-type">List</span>[<span class="hljs-type">U</span>]()<br><br>      <span class="hljs-keyword">while</span> (iterator.hasNext) {<br>        gets.add(makeGet(iterator.next))<br><br>        <span class="hljs-keyword">if</span> (gets.size() == batchSize) {<br>          <span class="hljs-keyword">var</span> results = htable.get(gets)<br>          res = res ++ results.map(convertResult)<br>          gets.clear()<br>        }<br>      }<br>      <span class="hljs-keyword">if</span> (gets.size() &gt; <span class="hljs-number">0</span>) {<br>        <span class="hljs-keyword">val</span> results = htable.get(gets)<br>        res = res ++ results.map(convertResult)<br>        gets.clear()<br>      }<br>      htable.close()<br>      res.iterator<br>    }<br>  }<br><br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">   * Produces a ClassTag[T], which is actually just a casted ClassTag[AnyRef].</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * This method is used to keep ClassTags out of the external Java API, as the Java compiler</span><br><span class="hljs-comment">   * cannot produce them automatically. While this ClassTag-faking does please the compiler,</span><br><span class="hljs-comment">   * it can cause problems at runtime if the Scala API relies on ClassTags for correctness.</span><br><span class="hljs-comment">   *</span><br><span class="hljs-comment">   * Often, though, a ClassTag[AnyRef] will not lead to incorrect behavior, just worse performance</span><br><span class="hljs-comment">   * or security issues. For instance, an Array[AnyRef] can hold any type T, but may lose primitive</span><br><span class="hljs-comment">   * specialization.</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span>[spark]<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fakeClassTag</span></span>[<span class="hljs-type">T</span>]: <span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>] = <span class="hljs-type">ClassTag</span>.<span class="hljs-type">AnyRef</span>.asInstanceOf[<span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>]]<br>}<br></code></span></pre></div></td></tr></tbody></table></figure>

<h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><p><em>优点</em>：</p>
<ol>
<li>使用方便，直接套用工具即可</li>
<li>不需要考虑不必要的序列化等问题</li>
<li>Cloudera出品，质量有保证</li>
</ol>
<p><em>缺点</em>：</p>
<ol>
<li>适合只写一张表的场景</li>
<li>应用场景收限，特别是需要自己保存Kafka的Offsets到Zookeeper时</li>
<li>定制化程度较高，不适合博主的需求</li>
</ol>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%B0%83%E7%A0%94/">调研</a>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                      <a class="hover-with-bg" href="/tags/Kafka/">Kafka</a>
                    
                      <a class="hover-with-bg" href="/tags/Hbase/">Hbase</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2018/06/29/Git-CI-CD%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%E3%80%90%E4%BA%8C%E3%80%91/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Git-CI/CD安装与使用【二】</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2018/06/29/Kafka-SparkStreaming-Hbase%E3%80%90%E4%BA%8C%E3%80%91/">
                        <span class="hidden-mobile">Kafka-&gt;SparkStreaming-&gt;Hbase【二】</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">×</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script>
<script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script>



<!-- Plugins -->


  
    
  



  <script defer="" src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script>
  







  <script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script>
  



  <script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script>
  



  <script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script>
  



  
  



  <script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  

  






















<script src="/bundle.js"></script><script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  ;

    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Kafka->SparkStreaming->Hbase【一】&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  ;

    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  ;

    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  ;

    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script></body></html>