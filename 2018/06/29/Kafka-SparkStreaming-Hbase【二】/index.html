<!DOCTYPE html><html lang="zh-CN"><head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="xinshiyou">
  <meta name="keywords" content="">
  <title>Kafka-&gt;SparkStreaming-&gt;Hbase【二】 - 星空捞月：找寻心中的安宁</title>

  


  
  

  
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->










<!-- 自定义样式保持在最底部 -->


  
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星空捞月：找寻心中的安宁" type="application/atom+xml">
<script>function loadCss(l){var d=document,h=d.head,s=d.createElement('link');s.rel='stylesheet';s.href=l;!function e(f){if (d.body)return f();setTimeout(function(){e(f)})}(function(){h.appendChild(s);});}loadCss('/style.css');loadCss('https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css');loadCss('https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css');loadCss('https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css');loadCss('//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css');loadCss('//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css');loadCss('https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css');</script><noscript><link rel="stylesheet" href="/style.css"><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"></noscript></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>星空捞月：找寻心中的安宁</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax="true" style="background: url('/img/top.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2018-06-29 16:52">
      2018年6月29日 下午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.9k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      55
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>  根据业务需求，将Kafka中数据抽取插入到Hbase中。目前网上可以找到许多相关的文章，这里介绍Github上的一个开源工具。</p>
<a id="more"></a>

<p>  上一章节讲到选择SparkOnHbase为主要原型，将之修改为我们需要的源代码。这里给出修改之后的源代码，修改之后符合我们的业务需求，并尽量避免引起其他不必要的问题。同时，后期优化程序执行效率问题。</p>
<h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight scala"><table><tbody><tr><td class="gutter"><div class="hljs"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></div></td><td class="code"><div class="hljs"><pre class=" language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params"></span></span><br><span class="hljs-class"><span class="hljs-params">  @transient sc:        <span class="hljs-type">SparkContext</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">  @transient config:    <span class="hljs-type">Configuration</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">  metas:                java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]],</span></span><br><span class="hljs-class"><span class="hljs-params">  val tmpHdfsConfgFile: <span class="hljs-type">String</span>                                                                                      = null</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Logging</span> </span>{<br><br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> tmpHdfsConfiguration: <span class="hljs-type">Configuration</span> = config<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> appliedCredentials = <span class="hljs-literal">false</span>;<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> metasLocal = metas<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> job = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(config)<br>  <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>  <span class="hljs-keyword">val</span> broadcastedConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(config))<br>  <span class="hljs-keyword">val</span> credentialsConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(job.getCredentials()))<br>  <span class="hljs-keyword">val</span> broadcastMetas = sc.broadcast(metas)<br><br>  <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span> &amp;&amp; config != <span class="hljs-literal">null</span>) {<br>    <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(config)<br>    <span class="hljs-keyword">val</span> tmpPath = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile)<br>    <span class="hljs-keyword">if</span> (!fs.exists(tmpPath)) {<br>      <span class="hljs-keyword">val</span> outputStream = fs.create(tmpPath)<br>      config.write(outputStream)<br>      outputStream.close();<br>    } <span class="hljs-keyword">else</span> {<br>      logWarning(<span class="hljs-string">"tmpHdfsConfigDir "</span> + tmpHdfsConfgFile + <span class="hljs-string">" exist!!"</span>)<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mapPartition</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>: <span class="hljs-type">ClassTag</span>](<br>    rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>    mp:  (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">R</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">R</span>] = {<br><br>    rdd.mapPartitions[<span class="hljs-type">R</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>](<br>      broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">applyCreds</span></span>[<span class="hljs-type">T</span>](configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]) {<br><br>    credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br><br>    logInfo(<span class="hljs-string">"appliedCredentials:"</span> + appliedCredentials + <span class="hljs-string">",credentials:"</span> + credentials);<br><br>    <span class="hljs-keyword">if</span> (appliedCredentials == <span class="hljs-literal">false</span> &amp;&amp; credentials != <span class="hljs-literal">null</span>) {<br>      appliedCredentials = <span class="hljs-literal">true</span><br>      logCredInformation(credentials)<br><br>      <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> ugi = <span class="hljs-type">UserGroupInformation</span>.getCurrentUser();<br>      ugi.addCredentials(credentials)<br>      ugi.setAuthenticationMethod(<span class="hljs-type">AuthenticationMethod</span>.<span class="hljs-type">PROXY</span>)<br>      ugi.addCredentials(credentialsConf.value.value)<br><br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logCredInformation</span></span>[<span class="hljs-type">T</span>](credentials2: <span class="hljs-type">Credentials</span>) {<br>    logInfo(<span class="hljs-string">"credentials:"</span> + credentials2);<br>    <span class="hljs-keyword">for</span> (a &lt;- <span class="hljs-number">0</span> until credentials2.getAllSecretKeys.size()) {<br>      logInfo(<span class="hljs-string">"getAllSecretKeys:"</span> + a + <span class="hljs-string">":"</span> + credentials2.getAllSecretKeys.get(a));<br>    }<br>    <span class="hljs-keyword">val</span> it = credentials2.getAllTokens.iterator();<br>    <span class="hljs-keyword">while</span> (it.hasNext) {<br>      logInfo(<span class="hljs-string">"getAllTokens:"</span> + it.next());<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkMutation</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], fun: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">DataEntity</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    <br>    rdd.foreachPartition(<br>      it =&gt; {<br>        hbaseForeachPartition[<span class="hljs-type">T</span>](<br>          broadcastedConf, broadcastMetas,<br>          it,<br>          (iter, hConnection, metas) =&gt; {<br><br>            iter.foreach(item =&gt; {<br><br>              <span class="hljs-keyword">val</span> entity = fun(item)<br>              <span class="hljs-keyword">val</span> dbName = entity.dbName<br>              <span class="hljs-keyword">val</span> tabName = entity.tabName<br>              <span class="hljs-keyword">if</span> (metas.containsKey(dbName) &amp;&amp; metas.get(dbName).containsKey(tabName)) {<br><br>                <span class="hljs-keyword">val</span> htable = hConnection.getTable(entity.dbName + <span class="hljs-string">":"</span> + entity.tabName)<br>                htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br><br>                entity.`<span class="hljs-class"><span class="hljs-keyword">type</span>` <span class="hljs-title">match</span> </span>{<br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"INSERT"</span> | <span class="hljs-string">"insert"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> insertPuts = <span class="hljs-type">Instance</span>.insert(entity, metas)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != insertPuts &amp;&amp; insertPuts.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(insertPuts)<br>                  }<br><br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"UPDATE"</span> | <span class="hljs-string">"update"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> updatePuts = <span class="hljs-type">Instance</span>.update(entity, metas)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != updatePuts &amp;&amp; updatePuts.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(updatePuts)<br>                  }<br><br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"DELETE"</span> | <span class="hljs-string">"delete"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> deleteDels = <span class="hljs-type">Instance</span>.delete(entity)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != deleteDels &amp;&amp; deleteDels.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(deleteDels)<br>                  }<br>                  <br>                  <span class="hljs-keyword">case</span> all: <span class="hljs-type">Any</span> =&gt; {<br>                    logInfo(<span class="hljs-string">"其他操作："</span> + all)<br>                  }<br>                }<br>                <br>                htable.flushCommits()<br>                htable.close()<br>              }<br>            })<br>          })<br>      })<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>[<span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>, f: ((<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">var</span> job: <span class="hljs-type">Job</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(getConf(broadcastedConf))<br><br>    <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>    <span class="hljs-type">TableMapReduceUtil</span>.initTableMapperJob(tableName, scan, classOf[<span class="hljs-type">IdentityTableMapper</span>], <span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, job)<br><br>    sc.newAPIHadoopRDD(<br>      job.getConfiguration(),<br>      classOf[<span class="hljs-type">TableInputFormat</span>],<br>      classOf[<span class="hljs-type">ImmutableBytesWritable</span>],<br>      classOf[<span class="hljs-type">Result</span>]).map(f)<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>(tableName: <span class="hljs-type">String</span>, scans: <span class="hljs-type">Scan</span>): <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    hbaseRDD[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])](<br>      tableName,<br>      scans,<br>      (r: (<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; {<br>        <span class="hljs-keyword">val</span> it = r._2.list().iterator()<br>        <span class="hljs-keyword">val</span> list = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])]()<br><br>        <span class="hljs-keyword">while</span> (it.hasNext()) {<br>          <span class="hljs-keyword">val</span> kv = it.next()<br>          list.add((kv.getFamily(), kv.getQualifier(), kv.getValue()))<br>        }<br><br>        (r._1.copyBytes(), list)<br>      })<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseForeachPartition</span></span>[<span class="hljs-type">T</span>](<br>    configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>    metasBroadcast:  <span class="hljs-type">Broadcast</span>[<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]],<br>    it:              <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>],<br>    fun:               (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]) =&gt; <span class="hljs-type">Unit</span>) = {<br>    <br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    <span class="hljs-keyword">val</span> metas = getMetas(metasBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    fun(it, hConnection, metas)<br>    hConnection.close()<br><br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * @desc get METAS from broadcast or driver's configure</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getMetas</span></span>(metasBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]]): <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]] = {<br><br>    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != metasLocal) {<br>      <span class="hljs-keyword">return</span> metasLocal<br>    } <span class="hljs-keyword">else</span> {<br>      <span class="hljs-keyword">try</span> {<br>        metasLocal = metasBroadcast.value<br>        metasLocal<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt; {<br>          logInfo(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br>    metasLocal<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConf</span></span>(configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]): <span class="hljs-type">Configuration</span> = {<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration != <span class="hljs-literal">null</span>) {<br>      tmpHdfsConfiguration<br>    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span>) {<br><br>      <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(<span class="hljs-type">SparkHadoopUtil</span>.get.conf)<br><br>      <span class="hljs-keyword">val</span> inputStream = fs.open(<span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile))<br>      tmpHdfsConfiguration = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>(<span class="hljs-literal">false</span>)<br>      tmpHdfsConfiguration.readFields(inputStream)<br>      inputStream.close()<br><br>      tmpHdfsConfiguration<br>    }<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration == <span class="hljs-literal">null</span>) {<br>      <span class="hljs-keyword">try</span> {<br>        tmpHdfsConfiguration = configBroadcast.value.value<br>        tmpHdfsConfiguration<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt; {<br>          println(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br><br>    tmpHdfsConfiguration<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseMapPartition</span></span>[<span class="hljs-type">K</span>, <span class="hljs-type">U</span>](<br>    configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>    it:              <span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>],<br>    mp:              (<span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    <span class="hljs-keyword">val</span> res = mp(it, hConnection)<br>    hConnection.close()<br>    res<br><br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetMapPartition</span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<span class="hljs-params"></span></span><br><span class="hljs-class"><span class="hljs-params">    tableName:     <span class="hljs-type">String</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">    batchSize:     <span class="hljs-type">Integer</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">    makeGet:       (<span class="hljs-type">T</span></span>) <span class="hljs-title">=&gt;</span> <span class="hljs-title">Get</span>,</span><br><span class="hljs-class">    <span class="hljs-title">convertResult</span></span>: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>) <span class="hljs-keyword">extends</span> <span class="hljs-type">Serializable</span> {<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(iterator: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], hConnection: <span class="hljs-type">HConnection</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br>      <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>      <span class="hljs-keyword">val</span> gets = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Get</span>]()<br>      <span class="hljs-keyword">var</span> res = <span class="hljs-type">List</span>[<span class="hljs-type">U</span>]()<br><br>      <span class="hljs-keyword">while</span> (iterator.hasNext) {<br>        gets.add(makeGet(iterator.next))<br><br>        <span class="hljs-keyword">if</span> (gets.size() == batchSize) {<br>          <span class="hljs-keyword">var</span> results = htable.get(gets)<br>          res = res ++ results.map(convertResult)<br>          gets.clear()<br>        }<br>      }<br><br>      <span class="hljs-keyword">if</span> (gets.size() &gt; <span class="hljs-number">0</span>) {<br>        <span class="hljs-keyword">val</span> results = htable.get(gets)<br>        res = res ++ results.map(convertResult)<br>        gets.clear()<br>      }<br><br>      htable.close()<br>      res.iterator<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fakeClassTag</span></span>[<span class="hljs-type">T</span>]: <span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>] = <span class="hljs-type">ClassTag</span>.<span class="hljs-type">AnyRef</span>.asInstanceOf[<span class="hljs-type">ClassTag</span>[<span class="hljs-type"><code class="language-hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HBaseContext</span>(<span class="hljs-params"></span></span><br><span class="hljs-class"><span class="hljs-params">  @transient sc:        <span class="hljs-type">SparkContext</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">  @transient config:    <span class="hljs-type">Configuration</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">  metas:                java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]],</span></span><br><span class="hljs-class"><span class="hljs-params">  val tmpHdfsConfgFile: <span class="hljs-type">String</span>                                                                                      = null</span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Logging</span> </span>{<br><br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> tmpHdfsConfiguration: <span class="hljs-type">Configuration</span> = config<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> appliedCredentials = <span class="hljs-literal">false</span>;<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">var</span> metasLocal = metas<br>  <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> job = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(config)<br>  <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>  <span class="hljs-keyword">val</span> broadcastedConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(config))<br>  <span class="hljs-keyword">val</span> credentialsConf = sc.broadcast(<span class="hljs-keyword">new</span> <span class="hljs-type">SerializableWritable</span>(job.getCredentials()))<br>  <span class="hljs-keyword">val</span> broadcastMetas = sc.broadcast(metas)<br><br>  <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span> &amp;&amp; config != <span class="hljs-literal">null</span>) {<br>    <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(config)<br>    <span class="hljs-keyword">val</span> tmpPath = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile)<br>    <span class="hljs-keyword">if</span> (!fs.exists(tmpPath)) {<br>      <span class="hljs-keyword">val</span> outputStream = fs.create(tmpPath)<br>      config.write(outputStream)<br>      outputStream.close();<br>    } <span class="hljs-keyword">else</span> {<br>      logWarning(<span class="hljs-string">"tmpHdfsConfigDir "</span> + tmpHdfsConfgFile + <span class="hljs-string">" exist!!"</span>)<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mapPartition</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>: <span class="hljs-type">ClassTag</span>](<br>    rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],<br>    mp:  (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">R</span>]): <span class="hljs-type">RDD</span>[<span class="hljs-type">R</span>] = {<br><br>    rdd.mapPartitions[<span class="hljs-type">R</span>](it =&gt; hbaseMapPartition[<span class="hljs-type">T</span>, <span class="hljs-type">R</span>](<br>      broadcastedConf,<br>      it,<br>      mp), <span class="hljs-literal">true</span>)<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">applyCreds</span></span>[<span class="hljs-type">T</span>](configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]) {<br><br>    credentials = <span class="hljs-type">SparkHadoopUtil</span>.get.getCurrentUserCredentials()<br><br>    logInfo(<span class="hljs-string">"appliedCredentials:"</span> + appliedCredentials + <span class="hljs-string">",credentials:"</span> + credentials);<br><br>    <span class="hljs-keyword">if</span> (appliedCredentials == <span class="hljs-literal">false</span> &amp;&amp; credentials != <span class="hljs-literal">null</span>) {<br>      appliedCredentials = <span class="hljs-literal">true</span><br>      logCredInformation(credentials)<br><br>      <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> ugi = <span class="hljs-type">UserGroupInformation</span>.getCurrentUser();<br>      ugi.addCredentials(credentials)<br>      ugi.setAuthenticationMethod(<span class="hljs-type">AuthenticationMethod</span>.<span class="hljs-type">PROXY</span>)<br>      ugi.addCredentials(credentialsConf.value.value)<br><br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">logCredInformation</span></span>[<span class="hljs-type">T</span>](credentials2: <span class="hljs-type">Credentials</span>) {<br>    logInfo(<span class="hljs-string">"credentials:"</span> + credentials2);<br>    <span class="hljs-keyword">for</span> (a &lt;- <span class="hljs-number">0</span> until credentials2.getAllSecretKeys.size()) {<br>      logInfo(<span class="hljs-string">"getAllSecretKeys:"</span> + a + <span class="hljs-string">":"</span> + credentials2.getAllSecretKeys.get(a));<br>    }<br>    <span class="hljs-keyword">val</span> it = credentials2.getAllTokens.iterator();<br>    <span class="hljs-keyword">while</span> (it.hasNext) {<br>      logInfo(<span class="hljs-string">"getAllTokens:"</span> + it.next());<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bulkMutation</span></span>[<span class="hljs-type">T</span>](rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>], fun: (<span class="hljs-type">T</span>) =&gt; (<span class="hljs-type">DataEntity</span>), autoFlush: <span class="hljs-type">Boolean</span>) {<br>    <br>    rdd.foreachPartition(<br>      it =&gt; {<br>        hbaseForeachPartition[<span class="hljs-type">T</span>](<br>          broadcastedConf, broadcastMetas,<br>          it,<br>          (iter, hConnection, metas) =&gt; {<br><br>            iter.foreach(item =&gt; {<br><br>              <span class="hljs-keyword">val</span> entity = fun(item)<br>              <span class="hljs-keyword">val</span> dbName = entity.dbName<br>              <span class="hljs-keyword">val</span> tabName = entity.tabName<br>              <span class="hljs-keyword">if</span> (metas.containsKey(dbName) &amp;&amp; metas.get(dbName).containsKey(tabName)) {<br><br>                <span class="hljs-keyword">val</span> htable = hConnection.getTable(entity.dbName + <span class="hljs-string">":"</span> + entity.tabName)<br>                htable.setAutoFlush(autoFlush, <span class="hljs-literal">true</span>)<br><br>                entity.`<span class="hljs-class"><span class="hljs-keyword">type</span>` <span class="hljs-title">match</span> </span>{<br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"INSERT"</span> | <span class="hljs-string">"insert"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> insertPuts = <span class="hljs-type">Instance</span>.insert(entity, metas)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != insertPuts &amp;&amp; insertPuts.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(insertPuts)<br>                  }<br><br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"UPDATE"</span> | <span class="hljs-string">"update"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> updatePuts = <span class="hljs-type">Instance</span>.update(entity, metas)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != updatePuts &amp;&amp; updatePuts.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(updatePuts)<br>                  }<br><br>                  <span class="hljs-keyword">case</span> <span class="hljs-string">"DELETE"</span> | <span class="hljs-string">"delete"</span> =&gt; {<br>                    <span class="hljs-keyword">val</span> deleteDels = <span class="hljs-type">Instance</span>.delete(entity)<br>                    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != deleteDels &amp;&amp; deleteDels.size() &gt; <span class="hljs-number">0</span>)<br>                      htable.batch(deleteDels)<br>                  }<br>                  <br>                  <span class="hljs-keyword">case</span> all: <span class="hljs-type">Any</span> =&gt; {<br>                    logInfo(<span class="hljs-string">"其他操作："</span> + all)<br>                  }<br>                }<br>                <br>                htable.flushCommits()<br>                htable.close()<br>              }<br>            })<br>          })<br>      })<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>[<span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](tableName: <span class="hljs-type">String</span>, scan: <span class="hljs-type">Scan</span>, f: ((<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; <span class="hljs-type">U</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">var</span> job: <span class="hljs-type">Job</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Job</span>(getConf(broadcastedConf))<br><br>    <span class="hljs-type">TableMapReduceUtil</span>.initCredentials(job)<br>    <span class="hljs-type">TableMapReduceUtil</span>.initTableMapperJob(tableName, scan, classOf[<span class="hljs-type">IdentityTableMapper</span>], <span class="hljs-literal">null</span>, <span class="hljs-literal">null</span>, job)<br><br>    sc.newAPIHadoopRDD(<br>      job.getConfiguration(),<br>      classOf[<span class="hljs-type">TableInputFormat</span>],<br>      classOf[<span class="hljs-type">ImmutableBytesWritable</span>],<br>      classOf[<span class="hljs-type">Result</span>]).map(f)<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseRDD</span></span>(tableName: <span class="hljs-type">String</span>, scans: <span class="hljs-type">Scan</span>): <span class="hljs-type">RDD</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])] = {<br><br>    hbaseRDD[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], java.util.<span class="hljs-type">List</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])])](<br>      tableName,<br>      scans,<br>      (r: (<span class="hljs-type">ImmutableBytesWritable</span>, <span class="hljs-type">Result</span>)) =&gt; {<br>        <span class="hljs-keyword">val</span> it = r._2.list().iterator()<br>        <span class="hljs-keyword">val</span> list = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[(<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>])]()<br><br>        <span class="hljs-keyword">while</span> (it.hasNext()) {<br>          <span class="hljs-keyword">val</span> kv = it.next()<br>          list.add((kv.getFamily(), kv.getQualifier(), kv.getValue()))<br>        }<br><br>        (r._1.copyBytes(), list)<br>      })<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseForeachPartition</span></span>[<span class="hljs-type">T</span>](<br>    configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>    metasBroadcast:  <span class="hljs-type">Broadcast</span>[<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]],<br>    it:              <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>],<br>    fun:               (<span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], <span class="hljs-type">HConnection</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]) =&gt; <span class="hljs-type">Unit</span>) = {<br>    <br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    <span class="hljs-keyword">val</span> metas = getMetas(metasBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    fun(it, hConnection, metas)<br>    hConnection.close()<br><br>  }<br><br>  <span class="hljs-comment">/**</span><br><span class="hljs-comment">   * @desc get METAS from broadcast or driver's configure</span><br><span class="hljs-comment">   */</span><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getMetas</span></span>(metasBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]]]): <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]] = {<br><br>    <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> != metasLocal) {<br>      <span class="hljs-keyword">return</span> metasLocal<br>    } <span class="hljs-keyword">else</span> {<br>      <span class="hljs-keyword">try</span> {<br>        metasLocal = metasBroadcast.value<br>        metasLocal<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt; {<br>          logInfo(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br>    metasLocal<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConf</span></span>(configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]]): <span class="hljs-type">Configuration</span> = {<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration != <span class="hljs-literal">null</span>) {<br>      tmpHdfsConfiguration<br>    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (tmpHdfsConfgFile != <span class="hljs-literal">null</span>) {<br><br>      <span class="hljs-keyword">val</span> fs = <span class="hljs-type">FileSystem</span>.newInstance(<span class="hljs-type">SparkHadoopUtil</span>.get.conf)<br><br>      <span class="hljs-keyword">val</span> inputStream = fs.open(<span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(tmpHdfsConfgFile))<br>      tmpHdfsConfiguration = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>(<span class="hljs-literal">false</span>)<br>      tmpHdfsConfiguration.readFields(inputStream)<br>      inputStream.close()<br><br>      tmpHdfsConfiguration<br>    }<br><br>    <span class="hljs-keyword">if</span> (tmpHdfsConfiguration == <span class="hljs-literal">null</span>) {<br>      <span class="hljs-keyword">try</span> {<br>        tmpHdfsConfiguration = configBroadcast.value.value<br>        tmpHdfsConfiguration<br>      } <span class="hljs-keyword">catch</span> {<br>        <span class="hljs-keyword">case</span> ex: <span class="hljs-type">Exception</span> =&gt; {<br>          println(<span class="hljs-string">"Unable to getConfig from broadcast"</span>)<br>        }<br>      }<br>    }<br><br>    tmpHdfsConfiguration<br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hbaseMapPartition</span></span>[<span class="hljs-type">K</span>, <span class="hljs-type">U</span>](<br>    configBroadcast: <span class="hljs-type">Broadcast</span>[<span class="hljs-type">SerializableWritable</span>[<span class="hljs-type">Configuration</span>]],<br>    it:              <span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>],<br>    mp:              (<span class="hljs-type">Iterator</span>[<span class="hljs-type">K</span>], <span class="hljs-type">HConnection</span>) =&gt; <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>]): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br><br>    <span class="hljs-keyword">val</span> config = getConf(configBroadcast)<br>    applyCreds(configBroadcast)<br>    <span class="hljs-keyword">val</span> hConnection = <span class="hljs-type">HConnectionManager</span>.createConnection(config)<br>    <span class="hljs-keyword">val</span> res = mp(it, hConnection)<br>    hConnection.close()<br>    res<br><br>  }<br><br>  <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetMapPartition</span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>](<span class="hljs-params"></span></span><br><span class="hljs-class"><span class="hljs-params">    tableName:     <span class="hljs-type">String</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">    batchSize:     <span class="hljs-type">Integer</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">    makeGet:       (<span class="hljs-type">T</span></span>) <span class="hljs-title">=&gt;</span> <span class="hljs-title">Get</span>,</span><br><span class="hljs-class">    <span class="hljs-title">convertResult</span></span>: (<span class="hljs-type">Result</span>) =&gt; <span class="hljs-type">U</span>) <span class="hljs-keyword">extends</span> <span class="hljs-type">Serializable</span> {<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(iterator: <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>], hConnection: <span class="hljs-type">HConnection</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">U</span>] = {<br>      <span class="hljs-keyword">val</span> htable = hConnection.getTable(tableName)<br><br>      <span class="hljs-keyword">val</span> gets = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>[<span class="hljs-type">Get</span>]()<br>      <span class="hljs-keyword">var</span> res = <span class="hljs-type">List</span>[<span class="hljs-type">U</span>]()<br><br>      <span class="hljs-keyword">while</span> (iterator.hasNext) {<br>        gets.add(makeGet(iterator.next))<br><br>        <span class="hljs-keyword">if</span> (gets.size() == batchSize) {<br>          <span class="hljs-keyword">var</span> results = htable.get(gets)<br>          res = res ++ results.map(convertResult)<br>          gets.clear()<br>        }<br>      }<br><br>      <span class="hljs-keyword">if</span> (gets.size() &gt; <span class="hljs-number">0</span>) {<br>        <span class="hljs-keyword">val</span> results = htable.get(gets)<br>        res = res ++ results.map(convertResult)<br>        gets.clear()<br>      }<br><br>      htable.close()<br>      res.iterator<br>    }<br>  }<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fakeClassTag</span></span>[<span class="hljs-type">T</span>]: <span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>] = <span class="hljs-type">ClassTag</span>.<span class="hljs-type">AnyRef</span>.asInstanceOf[<span class="hljs-type">ClassTag</span>[<span class="hljs-type">T</span>]]<br><br>}<br></code></span></pre></div></td></tr></tbody></table></figure>
<p>  根据我们的需求，重构了HbaseContext的源代码，删除了不必要的程序代码，从源头上保证了程序适用于我们的应用场景。</p>
<h2 id="SparkSteaming代码"><a href="#SparkSteaming代码" class="headerlink" title="SparkSteaming代码"></a>SparkSteaming代码</h2><figure class="highlight scala"><table><tbody><tr><td class="gutter"><div class="hljs"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></div></td><td class="code"><div class="hljs"><pre class=" language-hljs scala">   <span class="hljs-comment">/** initialize ZK UTIL */</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> zkUtil = <span class="hljs-keyword">new</span> <span class="hljs-type">CuratorUtil</span>()<br><br>   <span class="hljs-comment">/** get initialize parameters */</span><br>   <span class="hljs-keyword">val</span> offsetPath = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_SPARK_PATH</span>)<br>   zkUtil.createZKNodePer(offsetPath, <span class="hljs-literal">null</span>)<br><br>   <span class="hljs-keyword">val</span> topic = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_TOPIC_NAME</span>)<br>   <span class="hljs-keyword">val</span> recTime = <span class="hljs-type">Integer</span>.parseInt(<span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_RECTCKE_TIME</span>))<br>   <span class="hljs-keyword">val</span> <span class="hljs-type">ZK_MYSQL_PATH</span> = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_NAMESPACE_MYSQL_TABLES</span>);<br>   <span class="hljs-keyword">val</span> brokerList = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_BROKER_LIST</span>);<br><br>   <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>     <span class="hljs-string">"metadata.broker.list"</span> -&gt; brokerList,<br>     <span class="hljs-string">"zookeeper.connect"</span> -&gt; <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_SERVER_LIST</span>),<br>     <span class="hljs-string">"group.id"</span> -&gt; <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_CONSUMER_GROUPID</span>))<br><br>   <span class="hljs-comment">/** initialize HBASE METAS for filter */</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-meta">@volatile</span> <span class="hljs-keyword">var</span> metas: java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]] = <span class="hljs-type">Instance</span>.paserMetas(zkUtil, <span class="hljs-type">ZK_MYSQL_PATH</span>)<br>   <span class="hljs-keyword">if</span> (metas.size() &lt; <span class="hljs-number">1</span>) {<br>     println(<span class="hljs-string">"load hbase tablem metas failed!"</span>)<br>     <span class="hljs-keyword">return</span> ;<br>   }<br><br>   <span class="hljs-comment">/**  initialize Context */</span><br>   <span class="hljs-comment">// configure</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>     .set(<span class="hljs-string">"spark.streaming.backpressure.enabled"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_BACK_ENABLED</span>)) <span class="hljs-comment">// 设置可以限制</span><br>     .set(<span class="hljs-string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_KAFKA_MAXRATE</span>)) <span class="hljs-comment">// 设置具体限制数量：records/SEC</span><br>     .set(<span class="hljs-string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_SHUTDOWN_GRACEFULLLY</span>)) <span class="hljs-comment">// 设置Gracefully stop</span><br>     .set(<span class="hljs-string">"serializer.class"</span>, <span class="hljs-string">"kafka.serializer.StringEncoder"</span>)<br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> hbaseConf = <span class="hljs-type">HBaseConfiguration</span>.create();<br>   hbaseConf.addResource(<span class="hljs-string">"/etc/hbase/conf.cloudera.hbase/hbase-site.xml"</span>)<br>   hbaseConf.addResource(<span class="hljs-string">"/etc/hbase/conf.cloudera.hbase/core-site.xml"</span>)<br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)<br>   <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(recTime));<br><br>   <span class="hljs-keyword">val</span> fromOffsets = readOffsetData(zkUtil, offsetPath, topic, brokerList, <span class="hljs-number">9092</span>)<br>   <span class="hljs-keyword">val</span> stream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">StringDecoder</span>, <span class="hljs-type">StringDecoder</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">String</span>)](ssc, kafkaParams, fromOffsets, (mmd: <span class="hljs-type">MessageAndMetadata</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]) =&gt; (mmd.key(), mmd.message()))<br><br>   stream.foreachRDD(rdd =&gt; {<br><br>     <span class="hljs-keyword">val</span> offsets = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>].offsetRanges.map { offset =&gt; (offset.partition, offset.fromOffset) }<br>     writeOffsetData(zkUtil, offsetPath, offsets)<br><br>     <span class="hljs-keyword">val</span> hbaseContext = <span class="hljs-keyword">new</span> <span class="hljs-type">HBaseContext</span>(sc, hbaseConf, metas)<br>     hbaseContext.bulkMutation(rdd.map(item =&gt; item._2), (<span class="hljs-type">KV</span>: <span class="hljs-type">String</span>) =&gt; {<br>       <span class="hljs-type">Instance</span>.parse(<span class="hljs-type">KV</span>)<br>     }, <span class="hljs-literal">false</span>)<br><br>   })<br><br>   <span class="hljs-comment">/** add gracefully stop control */</span><br>   <span class="hljs-type">Runtime</span>.getRuntime.addShutdownHook(<span class="hljs-keyword">new</span> <span class="hljs-type">Thread</span> {<br>     <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(): <span class="hljs-type">Unit</span> = {<br>       <span class="hljs-keyword">try</span> {<br>         zkUtil.close()<br>       } <span class="hljs-keyword">catch</span> {<br>         <span class="hljs-keyword">case</span> e: <span class="hljs-type">Exception</span> =&gt; {<br>         }<br>       }<br>       ssc.stop(<span class="hljs-literal">true</span>, <span class="hljs-literal">true</span>)<br>     }<br>   })<br><br>   <span class="hljs-comment">/** spark streaming start and wait termination */</span><br>   ssc.start()<br>   ssc.awaitTermination()<br><br> }<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">  * @desc read data from Zookeeper</span><br><span class="hljs-comment">  */</span><br> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">readOffsetData</span></span>(zkUtil: <span class="hljs-type">CuratorUtil</span>, offsetPath: <span class="hljs-type">String</span>, topic: <span class="hljs-type">String</span>, brokerList: <span class="hljs-type">String</span>, kafkaPort: <span class="hljs-type">Integer</span>): <span class="hljs-type">Map</span>[<span class="hljs-type">TopicAndPartition</span>, <span class="hljs-type">Long</span>] = {<br><br>   <span class="hljs-keyword">val</span> orgData = zkUtil.readDataForPath(offsetPath)<br>   <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> == orgData) {<br>     <span class="hljs-keyword">val</span> util = <span class="hljs-type">KafkaUtil</span>.getInstance();<br>     util.init(brokerList, kafkaPort, topic);<br>     <span class="hljs-keyword">val</span> offsets = util.getLeastOffsets<br>     <span class="hljs-keyword">val</span> fromOffsets = <span class="hljs-keyword">for</span> (i &lt;- <span class="hljs-number">0</span> to offsets.size() - <span class="hljs-number">1</span>)<br>       <span class="hljs-keyword">yield</span> <span class="hljs-type">TopicAndPartition</span>.apply(topic, i) -&gt; offsets.get(i).toLong<br>     <span class="hljs-keyword">return</span> fromOffsets.toMap<br>   }<br><br>   <span class="hljs-keyword">val</span> data = <span class="hljs-type">JSON</span>.parseFull(orgData).get.asInstanceOf[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]]<br>   <span class="hljs-keyword">val</span> fromOffsets = data.map(item =&gt; {<br>     <span class="hljs-type">TopicAndPartition</span>.apply(topic, item._1.toInt) -&gt; item._2.toLong<br>   })<br>   <span class="hljs-keyword">return</span> fromOffsets<br><br> }<br><br> <span class="hljs-comment">/**</span><br><span class="hljs-comment">  * @desc write offset data to Zookeeper</span><br><span class="hljs-comment">  */</span><br> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">writeOffsetData</span></span>(zkUtil: <span class="hljs-type">CuratorUtil</span>, offsetPath: <span class="hljs-type">String</span>, data: <span class="hljs-type">Array</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">Long</span>)]): <span class="hljs-type">Unit</span> = {<br><br>   <span class="hljs-keyword">val</span> map = data.toMap[<span class="hljs-type">Int</span>, <span class="hljs-type">Long</span>].map(item =&gt; {<br>     item._1.toString() -&gt; item._2.toString()<br>   })<br>   zkUtil.setDataForPath(offsetPath, <span class="hljs-type"><code class="language-hljs scala">   <span class="hljs-comment">/** initialize ZK UTIL */</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> zkUtil = <span class="hljs-keyword">new</span> <span class="hljs-type">CuratorUtil</span>()<br><br>   <span class="hljs-comment">/** get initialize parameters */</span><br>   <span class="hljs-keyword">val</span> offsetPath = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_SPARK_PATH</span>)<br>   zkUtil.createZKNodePer(offsetPath, <span class="hljs-literal">null</span>)<br><br>   <span class="hljs-keyword">val</span> topic = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_TOPIC_NAME</span>)<br>   <span class="hljs-keyword">val</span> recTime = <span class="hljs-type">Integer</span>.parseInt(<span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_RECTCKE_TIME</span>))<br>   <span class="hljs-keyword">val</span> <span class="hljs-type">ZK_MYSQL_PATH</span> = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_NAMESPACE_MYSQL_TABLES</span>);<br>   <span class="hljs-keyword">val</span> brokerList = <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_BROKER_LIST</span>);<br><br>   <span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>     <span class="hljs-string">"metadata.broker.list"</span> -&gt; brokerList,<br>     <span class="hljs-string">"zookeeper.connect"</span> -&gt; <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">ZOOKEEPER_SERVER_LIST</span>),<br>     <span class="hljs-string">"group.id"</span> -&gt; <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">KAFKA_CONSUMER_GROUPID</span>))<br><br>   <span class="hljs-comment">/** initialize HBASE METAS for filter */</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-meta">@volatile</span> <span class="hljs-keyword">var</span> metas: java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, java.util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">ColumnInfo</span>]]] = <span class="hljs-type">Instance</span>.paserMetas(zkUtil, <span class="hljs-type">ZK_MYSQL_PATH</span>)<br>   <span class="hljs-keyword">if</span> (metas.size() &lt; <span class="hljs-number">1</span>) {<br>     println(<span class="hljs-string">"load hbase tablem metas failed!"</span>)<br>     <span class="hljs-keyword">return</span> ;<br>   }<br><br>   <span class="hljs-comment">/**  initialize Context */</span><br>   <span class="hljs-comment">// configure</span><br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>()<br>     .set(<span class="hljs-string">"spark.streaming.backpressure.enabled"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_BACK_ENABLED</span>)) <span class="hljs-comment">// 设置可以限制</span><br>     .set(<span class="hljs-string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_KAFKA_MAXRATE</span>)) <span class="hljs-comment">// 设置具体限制数量：records/SEC</span><br>     .set(<span class="hljs-string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="hljs-type">PropertiesUtil</span>.getProperty(<span class="hljs-type">ConstantUtil</span>.<span class="hljs-type">STREAMING_SHUTDOWN_GRACEFULLLY</span>)) <span class="hljs-comment">// 设置Gracefully stop</span><br>     .set(<span class="hljs-string">"serializer.class"</span>, <span class="hljs-string">"kafka.serializer.StringEncoder"</span>)<br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> hbaseConf = <span class="hljs-type">HBaseConfiguration</span>.create();<br>   hbaseConf.addResource(<span class="hljs-string">"/etc/hbase/conf.cloudera.hbase/hbase-site.xml"</span>)<br>   hbaseConf.addResource(<span class="hljs-string">"/etc/hbase/conf.cloudera.hbase/core-site.xml"</span>)<br>   <span class="hljs-meta">@transient</span> <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkContext</span>(sparkConf)<br>   <span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sc, <span class="hljs-type">Seconds</span>(recTime));<br><br>   <span class="hljs-keyword">val</span> fromOffsets = readOffsetData(zkUtil, offsetPath, topic, brokerList, <span class="hljs-number">9092</span>)<br>   <span class="hljs-keyword">val</span> stream = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">StringDecoder</span>, <span class="hljs-type">StringDecoder</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">String</span>)](ssc, kafkaParams, fromOffsets, (mmd: <span class="hljs-type">MessageAndMetadata</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]) =&gt; (mmd.key(), mmd.message()))<br><br>   stream.foreachRDD(rdd =&gt; {<br><br>     <span class="hljs-keyword">val</span> offsets = rdd.asInstanceOf[<span class="hljs-type">HasOffsetRanges</span>].offsetRanges.map { offset =&gt; (offset.partition, offset.fromOffset) }<br>     writeOffsetData(zkUtil, offsetPath, offsets)<br><br>     <span class="hljs-keyword">val</span> hbaseContext = <span class="hljs-keyword">new</span> <span class="hljs-type">HBaseContext</span>(sc, hbaseConf, metas)<br>     hbaseContext.bulkMutation(rdd.map(item =&gt; item._2), (<span class="hljs-type">KV</span>: <span class="hljs-type">String</span>) =&gt; {<br>       <span class="hljs-type">Instance</span>.parse(<span class="hljs-type">KV</span>)<br>     }, <span class="hljs-literal">false</span>)<br><br>   })<br><br>   <span class="hljs-comment">/** add gracefully stop control */</span><br>   <span class="hljs-type">Runtime</span>.getRuntime.addShutdownHook(<span class="hljs-keyword">new</span> <span class="hljs-type">Thread</span> {<br>     <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span></span>(): <span class="hljs-type">Unit</span> = {<br>       <span class="hljs-keyword">try</span> {<br>         zkUtil.close()<br>       } <span class="hljs-keyword">catch</span> {<br>         <span class="hljs-keyword">case</span> e: <span class="hljs-type">Exception</span> =&gt; {<br>         }<br>       }<br>       ssc.stop(<span class="hljs-literal">true</span>, <span class="hljs-literal">true</span>)<br>     }<br>   })<br><br>   <span class="hljs-comment">/** spark streaming start and wait termination */</span><br>   ssc.start()<br>   ssc.awaitTermination()<br><br> }<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">  * @desc read data from Zookeeper</span><br><span class="hljs-comment">  */</span><br> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">readOffsetData</span></span>(zkUtil: <span class="hljs-type">CuratorUtil</span>, offsetPath: <span class="hljs-type">String</span>, topic: <span class="hljs-type">String</span>, brokerList: <span class="hljs-type">String</span>, kafkaPort: <span class="hljs-type">Integer</span>): <span class="hljs-type">Map</span>[<span class="hljs-type">TopicAndPartition</span>, <span class="hljs-type">Long</span>] = {<br><br>   <span class="hljs-keyword">val</span> orgData = zkUtil.readDataForPath(offsetPath)<br>   <span class="hljs-keyword">if</span> (<span class="hljs-literal">null</span> == orgData) {<br>     <span class="hljs-keyword">val</span> util = <span class="hljs-type">KafkaUtil</span>.getInstance();<br>     util.init(brokerList, kafkaPort, topic);<br>     <span class="hljs-keyword">val</span> offsets = util.getLeastOffsets<br>     <span class="hljs-keyword">val</span> fromOffsets = <span class="hljs-keyword">for</span> (i &lt;- <span class="hljs-number">0</span> to offsets.size() - <span class="hljs-number">1</span>)<br>       <span class="hljs-keyword">yield</span> <span class="hljs-type">TopicAndPartition</span>.apply(topic, i) -&gt; offsets.get(i).toLong<br>     <span class="hljs-keyword">return</span> fromOffsets.toMap<br>   }<br><br>   <span class="hljs-keyword">val</span> data = <span class="hljs-type">JSON</span>.parseFull(orgData).get.asInstanceOf[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]]<br>   <span class="hljs-keyword">val</span> fromOffsets = data.map(item =&gt; {<br>     <span class="hljs-type">TopicAndPartition</span>.apply(topic, item._1.toInt) -&gt; item._2.toLong<br>   })<br>   <span class="hljs-keyword">return</span> fromOffsets<br><br> }<br><br> <span class="hljs-comment">/**</span><br><span class="hljs-comment">  * @desc write offset data to Zookeeper</span><br><span class="hljs-comment">  */</span><br> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">writeOffsetData</span></span>(zkUtil: <span class="hljs-type">CuratorUtil</span>, offsetPath: <span class="hljs-type">String</span>, data: <span class="hljs-type">Array</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">Long</span>)]): <span class="hljs-type">Unit</span> = {<br><br>   <span class="hljs-keyword">val</span> map = data.toMap[<span class="hljs-type">Int</span>, <span class="hljs-type">Long</span>].map(item =&gt; {<br>     item._1.toString() -&gt; item._2.toString()<br>   })<br>   zkUtil.setDataForPath(offsetPath, <span class="hljs-type">JSONObject</span>(map).toString)<br><br> }<br></code></span></pre></div></td></tr></tbody></table></figure>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%B0%83%E7%A0%94/">调研</a>
                    
                      <a class="hover-with-bg" href="/tags/Spark/">Spark</a>
                    
                      <a class="hover-with-bg" href="/tags/Kafka/">Kafka</a>
                    
                      <a class="hover-with-bg" href="/tags/Hbase/">Hbase</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2018/06/29/Kafka-SparkStreaming-Hbase%E3%80%90%E4%B8%80%E3%80%91/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Kafka-&gt;SparkStreaming-&gt;Hbase【一】</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2018/06/29/Maven%E4%BD%BF%E7%94%A8%E5%90%84%E7%A7%8D%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/">
                        <span class="hidden-mobile">Maven使用各种问题汇总</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">×</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script>
<script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script>



<!-- Plugins -->


  
    
  



  <script defer="" src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script>
  







  <script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script>
  



  <script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script>
  



  <script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script>
  



  
  



  <script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  

  






















<script src="/bundle.js"></script><script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  ;

    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Kafka->SparkStreaming->Hbase【二】&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  ;

    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  ;

    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  ;

    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script></body></html>